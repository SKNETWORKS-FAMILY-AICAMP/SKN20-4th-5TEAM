# -*- coding: utf-8 -*-
"""
LangGraph Agent ë° Tools ì •ì˜
ëŒ€í”¼ì†Œ ê²€ìƒ‰, ì¬ë‚œ í–‰ë™ìš”ë ¹, í†µê³„ ê¸°ëŠ¥ì„ ë‹´ë‹¹í•˜ëŠ” AI Agent
"""

import os
import requests
import json
import re
from math import radians, sin, cos, sqrt, atan2
from typing import TypedDict, Annotated, Optional
import time

from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_core.tools import tool
from langchain_core.messages import BaseMessage, HumanMessage, SystemMessage
from langchain_core.documents import Document
from langchain_community.retrievers import BM25Retriever
from langgraph.graph import StateGraph, END, START
from langgraph.graph.message import add_messages
from langgraph.checkpoint.memory import MemorySaver
from pathlib import Path
from dotenv import load_dotenv

# .env íŒŒì¼ ë¡œë“œ (í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê¸°ì¤€)
project_root = Path(__file__).parent.parent.parent
env_path = project_root / '.env'
load_dotenv(dotenv_path=env_path)

class EnsembleRetriever:
    """ê°„ë‹¨í•œ ì•™ìƒë¸” ë¦¬íŠ¸ë¦¬ë²„ êµ¬í˜„"""

    def __init__(self, retrievers, weights=None):
        self.retrievers = retrievers
        self.weights = weights or [1.0 / len(retrievers)] * len(retrievers)

    def invoke(self, query):
        all_docs = []
        for retriever, weight in zip(self.retrievers, self.weights):
            try:
                docs = retriever.invoke(query)
                for doc in docs:
                    doc.metadata["retriever_weight"] = weight
                    all_docs.append(doc)
            except:
                continue
        # ì¤‘ë³µ ì œê±° ë° ê°€ì¤‘ì¹˜ ê¸°ë°˜ ì •ë ¬
        seen = set()
        unique_docs = []
        for doc in all_docs:
            doc_id = doc.page_content[:100]
            if doc_id not in seen:
                seen.add(doc_id)
                unique_docs.append(doc)
        return unique_docs[:10]


def create_hybrid_retrievers(vectorstore):
    """í•˜ì´ë¸Œë¦¬ë“œ ë¦¬íŠ¸ë¦¬ë²„ ìƒì„± (Vector + BM25)"""
    if vectorstore is None:
        return None, None

    try:
        # 1. Vector Retriever
        shelter_vector_retriever = vectorstore.as_retriever(
            search_kwargs={"k": 5, "filter": {"type": "shelter"}}
        )
        guideline_vector_retriever = vectorstore.as_retriever(
            search_kwargs={"k": 3, "filter": {"type": "disaster_guideline"}}
        )

        # 2. BM25 Retriever ìƒì„±
        def create_bm25_retriever(doc_type: str):
            try:
                all_docs = vectorstore.get(where={"type": doc_type})
                if not all_docs or "documents" not in all_docs:
                    return None

                documents = []
                for i, text in enumerate(all_docs["documents"]):
                    metadata = (
                        all_docs["metadatas"][i] if "metadatas" in all_docs else {}
                    )
                    documents.append(Document(page_content=text, metadata=metadata))

                bm25_retriever = BM25Retriever.from_documents(documents)
                bm25_retriever.k = 5
                return bm25_retriever
            except Exception as e:
                print(f"âš ï¸ BM25 Retriever ìƒì„± ì‹¤íŒ¨ ({doc_type}): {e}")
                return None

        shelter_bm25 = create_bm25_retriever("shelter")
        guideline_bm25 = create_bm25_retriever("disaster_guideline")

        # 3. Ensemble (Hybrid) Retriever
        shelter_hybrid = EnsembleRetriever(
            retrievers=(
                [shelter_vector_retriever, shelter_bm25]
                if shelter_bm25
                else [shelter_vector_retriever]
            ),
            weights=[0.6, 0.4] if shelter_bm25 else [1.0],
        )

        guideline_hybrid = EnsembleRetriever(
            retrievers=(
                [guideline_vector_retriever, guideline_bm25]
                if guideline_bm25
                else [guideline_vector_retriever]
            ),
            weights=[0.7, 0.3] if guideline_bm25 else [1.0],
        )

        return shelter_hybrid, guideline_hybrid

    except Exception as e:
        print(f"âš ï¸ í•˜ì´ë¸Œë¦¬ë“œ ë¦¬íŠ¸ë¦¬ë²„ ìƒì„± ì‹¤íŒ¨: {e}")
        return None, None


def create_langgraph_app(vectorstore):
    """LangGraph Agent ìƒì„±"""

    # 1. LLM ì´ˆê¸°í™”
    llm = ChatOpenAI(model="gpt-4o-mini", temperature=0)
    llm_creative = ChatOpenAI(model="gpt-4o-mini", temperature=0.7)  # ì¼ë°˜ ì§€ì‹ìš©

    # 2. ì˜ë„ ë¶„ë¥˜ ì²´ì¸
    # 2. ì˜ë„ ë¶„ë¥˜ ì²´ì¸
    intent_classification_prompt = ChatPromptTemplate.from_messages(
        [
            (
                "system",
                """ë‹¹ì‹ ì€ ì‚¬ìš©ì ì§ˆë¬¸ì˜ ì˜ë„ë¥¼ ì •í™•í•˜ê²Œ ë¶„ë¥˜í•˜ëŠ” AIì…ë‹ˆë‹¤.

    ì§ˆë¬¸ì„ ë‹¤ìŒ ì¹´í…Œê³ ë¦¬ ì¤‘ í•˜ë‚˜ë¡œ ë¶„ë¥˜í•˜ì„¸ìš”:

    1. **hybrid_location_disaster**: ìœ„ì¹˜ + ì¬ë‚œ ìƒí™© ë³µí•© ì§ˆë¬¸ â­ ìš°ì„ ìˆœìœ„ 1
    - ì˜ˆ: "ì„¤ì•…ì‚° ê·¼ì²˜ì¸ë° ì‚°ì‚¬íƒœ ë°œìƒ ì‹œ", "ê°•ë‚¨ì—­ì—ì„œ ì§€ì§„ ë‚˜ë©´", "ëª…ë™ í™”ì¬"
    - í‚¤ì›Œë“œ: ì§€ëª… + (ì§€ì§„/í™”ì¬/ì‚°ì‚¬íƒœ/í™ìˆ˜ ë“±)

    2. **shelter_info**: íŠ¹ì • ëŒ€í”¼ì†Œì˜ ìƒì„¸ ì •ë³´ ì¡°íšŒ â­ ìƒˆë¡œ ì¶”ê°€
    - ì˜ˆ: "ë™ëŒ€ë¬¸ë§¨ì…˜ ìˆ˜ìš©ì¸ì›", "ì„œìš¸ì—­ ëŒ€í”¼ì†Œ ì •ë³´", "ë¡¯ë°ì›”ë“œ ìµœëŒ€ ìˆ˜ìš©"
    - í‚¤ì›Œë“œ: ì‹œì„¤ëª… + (ìˆ˜ìš©ì¸ì›/ì •ë³´/ë©´ì  ë“±)

    3. **shelter_search**: íŠ¹ì • ìœ„ì¹˜ì˜ ëŒ€í”¼ì†Œ ì°¾ê¸°
    - ì˜ˆ: "í•œë¼ì‚° ê·¼ì²˜ ëŒ€í”¼ì†Œ", "ê°•ë‚¨ì—­ ëŒ€í”¼ì†Œ"
    - í‚¤ì›Œë“œ: ì§€ëª… + (ê·¼ì²˜/ì£¼ë³€/ëŒ€í”¼ì†Œ) WITHOUT ì¬ë‚œ í‚¤ì›Œë“œ
    
    4. **shelter_count**: íŠ¹ì • ì¡°ê±´ì˜ ëŒ€í”¼ì†Œ ê°œìˆ˜ ì„¸ê¸°
    - ì˜ˆ: "ì„œìš¸ ëŒ€í”¼ì†Œ ê°œìˆ˜", "ì§€í•˜ ëŒ€í”¼ì†Œ ëª‡ ê°œ"
    
    5. **shelter_capacity**: ìˆ˜ìš©ì¸ì› ê¸°ì¤€ ëŒ€í”¼ì†Œ ì°¾ê¸°
    - ì˜ˆ: "ì²œ ëª… ì´ìƒ ìˆ˜ìš© ê°€ëŠ¥í•œ ëŒ€í”¼ì†Œ"
    - í‚¤ì›Œë“œ: ìˆ«ì + (ì´ìƒ/ì´í•˜/ìˆ˜ìš©)
    
    6. **disaster_guideline**: ì¬ë‚œ í–‰ë™ìš”ë ¹ë§Œ ì§ˆë¬¸
    - ì˜ˆ: "ì§€ì§„ ë°œìƒ ì‹œ í–‰ë™ìš”ë ¹" (ìœ„ì¹˜ ì •ë³´ ì—†ìŒ)
    
    7. **general_knowledge**: ì¬ë‚œ ê´€ë ¨ ì¼ë°˜ ì§€ì‹
    - ì˜ˆ: "ì§€ì§„ì´ ë­ì•¼", "ì“°ë‚˜ë¯¸ë€"
    
    8. **general_chat**: ì¼ë°˜ ëŒ€í™”
    - ì˜ˆ: "ì•ˆë…•", "ê³ ë§ˆì›Œ"

    **ì¤‘ìš” ìš°ì„ ìˆœìœ„**: 
    - "ìœ„ì¹˜ + ì¬ë‚œ"ì´ í•¨ê»˜ ìˆìœ¼ë©´ ë¬´ì¡°ê±´ **hybrid_location_disaster**
    - "ì‹œì„¤ëª… + ìˆ˜ìš©ì¸ì›/ì •ë³´"ëŠ” **shelter_info**
    - "ìœ„ì¹˜ + ê·¼ì²˜/ì£¼ë³€"ë§Œ ìˆê³  ì¬ë‚œ ì—†ìœ¼ë©´ **shelter_search**

    **ì‘ë‹µ í˜•ì‹**: JSON
    {{
        "intent": "ì¹´í…Œê³ ë¦¬ëª…",
        "confidence": 0.95,
        "reason": "ë¶„ë¥˜ ê·¼ê±°"
    }}""",
            ),
            ("user", "{query}"),
        ]
    )

    intent_chain = intent_classification_prompt | llm | StrOutputParser()

    # 3. ì§ˆë¬¸ ì¬ì •ì˜ ì²´ì¸ (ê²€ìƒ‰ ì •í™•ë„ í–¥ìƒ)
    query_rewrite_prompt = ChatPromptTemplate.from_messages(
        [
            (
                "system",
                """ë‹¹ì‹ ì€ ê²€ìƒ‰ ì¿¼ë¦¬ë¥¼ ìµœì í™”í•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤.

ì‚¬ìš©ìì˜ ì§ˆë¬¸ì„ **ê²€ìƒ‰ ì‹œìŠ¤í…œë³„ë¡œ ìµœì í™”**ëœ í˜•íƒœë¡œ ì¬ì‘ì„±í•˜ì„¸ìš”.

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

**1ï¸âƒ£ ì¹´ì¹´ì˜¤ APIìš© (ìœ„ì¹˜ ê²€ìƒ‰)**
- **ëª©ì **: ì •í™•í•œ ì¥ì†Œ ì¢Œí‘œ ì°¾ê¸°
- **ì›ì¹™**: 
  âœ… íŠ¹ì • ìœ„ì¹˜(ì—­, ê±´ë¬¼, ë§¤ì¥): ê·¸ëŒ€ë¡œ ìœ ì§€
     ì˜ˆ) "ê°•ë‚¨ì—­", "ë¡¯ë°ì›”ë“œ", "ìŠ¤íƒ€ë²…ìŠ¤ ëª…ë™ì "
  
  âœ… ì§€ì—­ëª…(ì‹œ/êµ¬/ë™): í–‰ì •ê¸°ê´€ìœ¼ë¡œ ë³€í™˜
     ì˜ˆ) "ì„œìš¸" â†’ "ì„œìš¸ì‹œì²­"
     ì˜ˆ) "ë™ì‘êµ¬" â†’ "ë™ì‘êµ¬ì²­"
     ì˜ˆ) "ì†¡íŒŒ" â†’ "ì†¡íŒŒêµ¬ì²­"
     ì˜ˆ) "ì—¬ì˜ë„ë™" â†’ "ì—¬ì˜ë„ë™ ì£¼ë¯¼ì„¼í„°"
  
  âœ… "ëŒ€í”¼ì†Œ", "ê·¼ì²˜", "ì£¼ë³€" ë“± ì œê±°
  
- **ì˜ˆì‹œ**:
  * "ê°•ë‚¨ì—­ ê·¼ì²˜ ëŒ€í”¼ì†Œ" â†’ "ê°•ë‚¨ì—­"
  * "ì„œìš¸ ëŒ€í”¼ì†Œ" â†’ "ì„œìš¸ì‹œì²­"
  * "ë™ì‘êµ¬ ì£¼ë³€" â†’ "ë™ì‘êµ¬ì²­"
  * "ì†¡íŒŒ ì§€í•˜ ëŒ€í”¼ì†Œ" â†’ "ì†¡íŒŒêµ¬ì²­"

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

**2ï¸âƒ£ VectorDBìš© (ì˜ë¯¸ ê²€ìƒ‰)**
- **ëª©ì **: ìœ ì‚¬í•œ ë¬¸ì„œ ì°¾ê¸° (BM25 + Vector)
- **ì›ì¹™**:
  âœ… í•µì‹¬ í‚¤ì›Œë“œ + ë™ì˜ì–´ ì¶”ê°€
  âœ… ì§€ì—­ëª… ë‹¤ì–‘í•œ í‘œí˜„ (ì„œìš¸ â†’ ì„œìš¸ ì„œìš¸ì‹œ ì„œìš¸íŠ¹ë³„ì‹œ)
  âœ… ìœ„ì¹˜ ìœ í˜• ëª…í™•í™” (ì§€í•˜ â†’ ì§€í•˜ ì§€í•˜ì¸µ)
  âœ… ìµœëŒ€ 10ë‹¨ì–´ ì´ë‚´
  
- **ì˜ˆì‹œ**:
  * "ê°•ë‚¨ì—­ ê·¼ì²˜ ëŒ€í”¼ì†Œ" â†’ "ê°•ë‚¨ì—­ ê°•ë‚¨ ëŒ€í”¼ì†Œ í”¼ë‚œì²˜"
  * "ì„œìš¸ ëŒ€í”¼ì†Œ" â†’ "ì„œìš¸ ì„œìš¸ì‹œ ì„œìš¸íŠ¹ë³„ì‹œ ëŒ€í”¼ì†Œ"
  * "ë™ì‘êµ¬ ì§€í•˜" â†’ "ë™ì‘êµ¬ ë™ì‘ ì§€í•˜ ì§€í•˜ì¸µ ëŒ€í”¼ì†Œ"

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

**ì‘ë‹µ í˜•ì‹** (JSON):
{{
    "kakao": "ì¹´ì¹´ì˜¤ APIìš© ì¿¼ë¦¬",
    "vector": "VectorDBìš© ì¿¼ë¦¬",
    "location_type": "specific" or "region"
}}

**location_type íŒë‹¨ ê¸°ì¤€**:
- "specific": ì—­ëª…, ê±´ë¬¼ëª…, ë§¤ì¥ëª… ë“± êµ¬ì²´ì  ì¥ì†Œ
- "region": ì‹œ/êµ¬/ë™ ë“± í–‰ì •êµ¬ì—­""",
            ),
            ("user", "{original_query}"),
        ]
    )

    query_rewrite_chain = query_rewrite_prompt | llm | StrOutputParser()

    # 4. í•˜ì´ë¸Œë¦¬ë“œ ë¦¬íŠ¸ë¦¬ë²„ ìƒì„±
    shelter_hybrid, guideline_hybrid = create_hybrid_retrievers(vectorstore)

    # 5. Tools ì •ì˜
    @tool
    def search_shelter_by_location(query: str) -> dict:
        """
        íŠ¹ì • ìœ„ì¹˜ì˜ ëŒ€í”¼ì†Œë¥¼ ê²€ìƒ‰í•©ë‹ˆë‹¤.
        - íŠ¹ì • ì¥ì†Œ(ì—­, ê±´ë¬¼): í•´ë‹¹ ìœ„ì¹˜ ì¤‘ì‹¬ìœ¼ë¡œ ê²€ìƒ‰
        - ì§€ì—­ëª…(ì‹œ/êµ¬): í–‰ì •ê¸°ê´€(ì‹œì²­/êµ¬ì²­) ì¤‘ì‹¬ìœ¼ë¡œ ê²€ìƒ‰
        """
        start_time = time.time()
        
        try:
            # â­ ì§ˆë¬¸ ì¬ì •ì˜ë¡œ location_type íŒë‹¨
            vector_query = query_rewrite_chain.invoke({"original_query": query})
            
            try:
                import json
                parsed = json.loads(vector_query)
                kakao_query = parsed.get("kakao", query)
                vector_query = parsed.get("vector", query)
                location_type = parsed.get("location_type", "specific")
                
                print(f"[search_shelter_by_location] ìœ„ì¹˜ ìœ í˜•: {location_type}")
                print(f"[search_shelter_by_location] ì¹´ì¹´ì˜¤ìš©: '{kakao_query}'")
                print(f"[search_shelter_by_location] Vectorìš©: '{vector_query}'")
                
            except:
                # JSON íŒŒì‹± ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ê°’
                kakao_query = query
                location_type = "specific"
                
                # ê¸°ì¡´ ì •ì œ ë¡œì§
                remove_words = ["ê·¼ì²˜", "ì£¼ë³€", "ì¸ê·¼", "ëŒ€í”¼ì†Œ", "í”¼ë‚œì†Œ", "í”¼ë‚œì²˜", 
                              "ì•Œë ¤ì¤˜", "ì°¾ì•„ì¤˜", "ì–´ë””", "ìˆì–´", "ì˜", "ë¥¼", "ì„"]
                for word in remove_words:
                    kakao_query = kakao_query.replace(word, "")
                
                kakao_query = " ".join(kakao_query.split()).strip()
        
            # ì¹´ì¹´ì˜¤ API í˜¸ì¶œ
            api_start = time.time()
            kakao_api_key = os.getenv("KAKAO_REST_API_KEY")
            if not kakao_api_key:
                return {"text": "ì¹´ì¹´ì˜¤ API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.", "structured_data": None}

            headers = {"Authorization": f"KakaoAK {kakao_api_key}"}
            url = "https://dapi.kakao.com/v2/local/search/keyword.json"
            params = {"query": kakao_query}

            try:
                response = requests.get(url, headers=headers, params=params)
                data = response.json()

                if not data.get("documents"):
                    return {
                        "text": f"'{kakao_query}' ìœ„ì¹˜ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.",
                        "structured_data": None,
                    }

                place = data["documents"][0]
                user_lat = float(place["y"])
                user_lon = float(place["x"])
                place_name = place["place_name"]
                
                location_desc = f"{place_name} ({location_type})"
                print(f"[ì¹´ì¹´ì˜¤ API] ì¥ì†Œ í™•ì¸: {location_desc} ({user_lat}, {user_lon})")

            except Exception as e:
                print(f"[ì¹´ì¹´ì˜¤ API ì˜¤ë¥˜] {e}")
                return {
                    "text": f"ì¹´ì¹´ì˜¤ API í˜¸ì¶œ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}",
                    "structured_data": None,
                }
            
            api_time = time.time() - api_start
            print(f"â±ï¸ [ì¹´ì¹´ì˜¤ API í˜¸ì¶œ ì‹œê°„] {api_time:.3f}ì´ˆ")
            
            # VectorDB ê²€ìƒ‰ (ê¸°ì¡´ ë¡œì§)
            vector_start = time.time()
            all_data = vectorstore.get(where={"type": "shelter"})
            vector_time = time.time() - vector_start
            print(f"â±ï¸ [ChromaDB ê²€ìƒ‰ ì‹œê°„] {vector_time:.3f}ì´ˆ")
            
            # ê±°ë¦¬ ê³„ì‚°
            calc_start = time.time()
            def haversine(lat1, lon1, lat2, lon2):
                R = 6371
                dlat = radians(lat2 - lat1)
                dlon = radians(lon2 - lon1)
                a = (
                    sin(dlat / 2) ** 2
                    + cos(radians(lat1)) * cos(radians(lat2)) * sin(dlon / 2) ** 2
                )
                c = 2 * atan2(sqrt(a), sqrt(1 - a))
                return R * c

            shelters = []

            for metadata in all_data["metadatas"]:
                try:
                    lat = float(metadata.get("lat", 0))
                    lon = float(metadata.get("lon", 0))
                    if lat == 0 or lon == 0:
                        continue

                    distance = haversine(user_lat, user_lon, lat, lon)
                    shelters.append(
                        {
                            "name": metadata.get("facility_name", "N/A"),
                            "address": metadata.get("address", "N/A"),
                            "lat": lat,
                            "lon": lon,
                            "distance": distance,
                            "capacity": int(metadata.get("capacity", 0)),
                            "shelter_type": metadata.get("shelter_type", "N/A"),
                            "facility_type": metadata.get("facility_type", "N/A"),
                        }
                    )
                except Exception:
                    continue

            # ê±°ë¦¬ìˆœ ì •ë ¬
            shelters.sort(key=lambda x: x["distance"])
            top_5 = shelters[:5]

            if not top_5:
                return {
                    "text": f"'{place_name}' ê·¼ì²˜ì— ëŒ€í”¼ì†Œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.",
                    "structured_data": None,
                }

            # í…ìŠ¤íŠ¸ ê²°ê³¼ í¬ë§·íŒ…
            location_text = "ì§€ì—­" if location_type == "region" else "ìœ„ì¹˜"
            result_text = f"ğŸ“ **{place_name}** {location_text} ê¸°ì¤€ ëŒ€í”¼ì†Œ {len(top_5)}ê³³\n\n"
            for i, s in enumerate(top_5, 1):
                result_text += f"{i}. **{s['name']}**\n"
                result_text += f"   ğŸ“ ê±°ë¦¬: {s['distance']:.2f}km\n"
                result_text += f"   ğŸ“ ì£¼ì†Œ: {s['address']}\n"
                result_text += f"   ğŸ“ ìœ„ì¹˜: {s['shelter_type']}\n"
                result_text += f"   ğŸ“ ìˆ˜ìš©ì¸ì›: {s['capacity']:,}ëª…\n\n"

            # êµ¬ì¡°í™”ëœ ë°ì´í„° (ì§€ë„ í‘œì‹œìš©)
            structured_data = {
                "location": place_name,
                "location_type": location_type,  # NEW
                "user_coordinates": [user_lat, user_lon],
                "coordinates": [user_lat, user_lon],
                "shelters": top_5,
                "total_count": len(all_data["metadatas"]),
            }

            total_time = time.time() - start_time
            print(f"â±ï¸ [search_shelter_by_location ì´ ì‹œê°„] {total_time:.3f}ì´ˆ")
            
            return {"text": result_text.strip(), "structured_data": structured_data}

        except Exception as e:
            print(f"[ERROR] search_shelter_by_location: {e}")
            import traceback
            traceback.print_exc()
            return {"text": f"ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}", "structured_data": None}

    @tool
    def count_shelters(query: str) -> dict:
        """
        íŠ¹ì • ì¡°ê±´(ì§€ì—­, ìœ„ì¹˜ìœ í˜• ë“±)ì— ë§ëŠ” ëŒ€í”¼ì†Œ ê°œìˆ˜ë¥¼ ì…‰ë‹ˆë‹¤.
        ì§€ë„ í‘œì‹œìš© êµ¬ì¡°í™”ëœ ë°ì´í„°ë¥¼ í¬í•¨í•©ë‹ˆë‹¤.

        Args:
            query: ê²€ìƒ‰ ì¡°ê±´ (ì˜ˆ: "ì„œìš¸ ì§€í•˜", "ë¶€ì‚° ë¯¼ë°©ìœ„")

        Returns:
            dict: {"text": str, "structured_data": dict} í˜•ì‹
        """
        try:
            # ì¿¼ë¦¬ ì¬ì •ì˜
            rewritten = query_rewrite_chain.invoke({"original_query": query})
            print(f"[count_shelters] ì¬ì •ì˜: {query} â†’ {rewritten}")

            if shelter_hybrid is None:
                return {
                    "text": "ê²€ìƒ‰ ì‹œìŠ¤í…œì´ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.",
                    "structured_data": None,
                }

            # 1ë‹¨ê³„: VectorDB ì „ì²´ì—ì„œ ë§¤ì¹­ë˜ëŠ” ëŒ€í”¼ì†Œ ì°¾ê¸° (ì „ì²´ ê°œìˆ˜ ì¹´ìš´íŠ¸ìš©)
            all_data = vectorstore.get(where={"type": "shelter"})
            all_shelters = []

            # ê²€ìƒ‰ í‚¤ì›Œë“œ ì¶”ì¶œ (ê³µë°±ìœ¼ë¡œ ë¶„ë¦¬)
            search_keywords = rewritten.lower().split()

            for metadata in all_data["metadatas"]:
                # ì‹œì„¤ëª…, ì£¼ì†Œ, ìœ„ì¹˜ìœ í˜•ì—ì„œ í‚¤ì›Œë“œ ê²€ìƒ‰
                facility_name = metadata.get("facility_name", "").lower()
                address = metadata.get("address", "").lower()
                shelter_type = metadata.get("shelter_type", "").lower()

                # ê²€ìƒ‰ ëŒ€ìƒ í…ìŠ¤íŠ¸ ê²°í•©
                search_text = f"{facility_name} {address} {shelter_type}"

                # ëª¨ë“  í‚¤ì›Œë“œ ì¤‘ í•˜ë‚˜ë¼ë„ í¬í•¨ë˜ë©´ ë§¤ì¹­
                if any(keyword in search_text for keyword in search_keywords):
                    all_shelters.append(
                        {
                            "name": metadata.get("facility_name", "N/A"),
                            "address": metadata.get("address", "N/A"),
                            "lat": float(metadata.get("lat", 0)),
                            "lon": float(metadata.get("lon", 0)),
                            "distance": 0,
                            "capacity": int(metadata.get("capacity", 0)),
                            "shelter_type": metadata.get("shelter_type", "N/A"),
                            "facility_type": metadata.get("facility_type", "N/A"),
                        }
                    )

            total_count = len(all_shelters)

            # 2ë‹¨ê³„: í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ìœ¼ë¡œ ìƒìœ„ ê²°ê³¼ ì¶”ì¶œ (ì§€ë„ í‘œì‹œìš©)
            results = shelter_hybrid.invoke(rewritten)

            # ì¤‘ë³µ ì œê±° ë° ëŒ€í”¼ì†Œ ì •ë³´ ìˆ˜ì§‘
            seen = set()
            top_shelters = []
            for doc in results:
                name = doc.metadata.get("facility_name", "")
                if name and name not in seen:
                    seen.add(name)
                    top_shelters.append(
                        {
                            "name": name,
                            "address": doc.metadata.get("address", "N/A"),
                            "lat": float(doc.metadata.get("lat", 0)),
                            "lon": float(doc.metadata.get("lon", 0)),
                            "distance": 0,
                            "capacity": int(doc.metadata.get("capacity", 0)),
                            "shelter_type": doc.metadata.get("shelter_type", "N/A"),
                            "facility_type": doc.metadata.get("facility_type", "N/A"),
                        }
                    )
                    if len(top_shelters) >= 10:  # ìµœëŒ€ 10ê°œ
                        break

            if total_count == 0:
                return {
                    "text": f"'{query}' ì¡°ê±´ì— ë§ëŠ” ëŒ€í”¼ì†Œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.",
                    "structured_data": None,
                }

            # ì¤‘ì‹¬ ì¢Œí‘œ ê³„ì‚° (í‰ê· )
            display_shelters = top_shelters if top_shelters else all_shelters[:10]
            avg_lat = (
                sum(s["lat"] for s in display_shelters if s["lat"] != 0)
                / len([s for s in display_shelters if s["lat"] != 0])
                if any(s["lat"] != 0 for s in display_shelters)
                else 0
            )
            avg_lon = (
                sum(s["lon"] for s in display_shelters if s["lon"] != 0)
                / len([s for s in display_shelters if s["lon"] != 0])
                if any(s["lon"] != 0 for s in display_shelters)
                else 0
            )

            structured_data = {
                "location": query,
                "coordinates": (avg_lat, avg_lon) if avg_lat != 0 else None,
                "shelters": display_shelters,  # ì§€ë„ì— í‘œì‹œí•  10ê°œ
                "total_count": total_count,  # VectorDB ì „ì²´ ë§¤ì¹­ ê°œìˆ˜
            }

            return {
                "text": f"**'{query}'** ì¡°ê±´ì— ë§ëŠ” ëŒ€í”¼ì†ŒëŠ” ì´ **{total_count}ê°œ**ì…ë‹ˆë‹¤. ğŸ“Š",
                "structured_data": structured_data,
            }

        except Exception as e:
            print(f"[ERROR] count_shelters: {e}")
            import traceback

            traceback.print_exc()
            return {"text": f"ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}", "structured_data": None}

    @tool
    def search_shelter_by_capacity(query: str) -> dict:
        """
        ìˆ˜ìš©ì¸ì› ê¸°ì¤€ìœ¼ë¡œ ëŒ€í”¼ì†Œë¥¼ ê²€ìƒ‰í•©ë‹ˆë‹¤.
        ìœ„ì¹˜ ì¡°ê±´ì´ ìˆìœ¼ë©´ í•´ë‹¹ ì§€ì—­ ë‚´ì—ì„œë§Œ ê²€ìƒ‰í•©ë‹ˆë‹¤.
        "ì´ìƒ"ê³¼ "ì´í•˜"ë¥¼ êµ¬ë¶„í•˜ì—¬ í•„í„°ë§í•©ë‹ˆë‹¤.

        Args:
            query: ìˆ˜ìš©ì¸ì› ì¡°ê±´ (ì˜ˆ: "ì²œ ëª… ì´ìƒ", "300ëª… ì´í•˜", "ì„œìš¸ ë™ì‘êµ¬ ì²œëª… ì´ìƒ")

        Returns:
            dict: {"text": str, "structured_data": dict} í˜•ì‹
        """
        try:
            # 1ë‹¨ê³„: "ì´ìƒ" vs "ì´í•˜" íŒë‹¨
            is_minimum = True  # ê¸°ë³¸ê°’: ì´ìƒ
            if "ì´í•˜" in query:
                is_minimum = False

            # 2ë‹¨ê³„: ìˆ«ì ë‹¨ìœ„ ë¨¼ì € ì²˜ë¦¬ (ì²œ, ë§Œ)
            capacity_value = 0

            # "ì²œëª…", "ì²œ ëª…", "1ì²œëª…" ë“± ì²˜ë¦¬
            if "ì²œ" in query or "1000" in query:
                # ì²œ ì•ì˜ ìˆ«ì ì°¾ê¸°
                thousand_pattern = re.search(r"(\d+)\s*ì²œ", query)
                if thousand_pattern:
                    capacity_value = int(thousand_pattern.group(1)) * 1000
                else:
                    capacity_value = 1000  # ìˆ«ì ì—†ì´ "ì²œëª…"ë§Œ ìˆëŠ” ê²½ìš°
            elif "ë§Œ" in query or "10000" in query:
                # ë§Œ ì•ì˜ ìˆ«ì ì°¾ê¸°
                ten_thousand_pattern = re.search(r"(\d+)\s*ë§Œ", query)
                if ten_thousand_pattern:
                    capacity_value = int(ten_thousand_pattern.group(1)) * 10000
                else:
                    capacity_value = 10000  # ìˆ«ì ì—†ì´ "ë§Œëª…"ë§Œ ìˆëŠ” ê²½ìš°
            else:
                # ì¼ë°˜ ìˆ«ì ì¶”ì¶œ
                numbers = re.findall(r"\d+", query)
                if numbers:
                    capacity_value = int(numbers[0])

            if capacity_value == 0:
                return {
                    "text": "ìˆ˜ìš©ì¸ì›ì„ ëª…í™•íˆ ì…ë ¥í•´ì£¼ì„¸ìš”. (ì˜ˆ: 1000ëª… ì´ìƒ, ì²œëª… ì´ìƒ)",
                    "structured_data": None,
                }

            # 2ë‹¨ê³„: ìœ„ì¹˜ í‚¤ì›Œë“œ ì¶”ì¶œ
            location_query = query

            # ìˆ˜ìš©ì¸ì› ê´€ë ¨ ë¶€ë¶„ ì™„ì „ ì œê±°
            remove_patterns = [
                r"\d+\s*ì²œ\s*ëª…?\s*(ì´ìƒ|ì´í•˜)?",  # "1ì²œëª… ì´ìƒ", "ì²œëª…"
                r"\d+\s*ë§Œ\s*ëª…?\s*(ì´ìƒ|ì´í•˜)?",  # "1ë§Œëª… ì´ìƒ", "ë§Œëª…"
                r"\d+\s*ëª…\s*(ì´ìƒ|ì´í•˜)?",  # "53600ëª… ì´ìƒ", "1000ëª…"
                r"ì²œ\s*ëª…?\s*(ì´ìƒ|ì´í•˜)?",  # "ì²œëª… ì´ìƒ"
                r"ë§Œ\s*ëª…?\s*(ì´ìƒ|ì´í•˜)?",  # "ë§Œëª… ì´ìƒ"
                r"ìˆ˜ìš©\s*ì¸ì›\s*(ì´|ê°€)?",  # "ìˆ˜ìš©ì¸ì›ì´", "ìˆ˜ìš©ì¸ì›"
                r"ìˆ˜ìš©\s*í• ?\s*ìˆ˜\s*ìˆëŠ”",  # "ìˆ˜ìš©í•  ìˆ˜ ìˆëŠ”"
                r"ìˆ˜ìš©\s*ê°€ëŠ¥í•œ?",  # "ìˆ˜ìš©ê°€ëŠ¥í•œ"
                r"ìµœëŒ€\s*ìˆ˜ìš©",  # "ìµœëŒ€ìˆ˜ìš©"
                r"ì¸ì›\s*(ì´|ê°€|ì„|ë¥¼)?",  # "ì¸ì›ì´", "ì¸ì›ì„"
                r"ëŒ€í”¼ì†Œ\s*(ë¥¼|ì„|ì´|ê°€)?",  # "ëŒ€í”¼ì†Œë¥¼", "ëŒ€í”¼ì†Œ"
                r"ì°¾ì•„\s*ì¤˜?",
                r"ì•Œë ¤\s*ì¤˜?",
                r"ìˆì–´\??",
                r"ìˆë‹ˆ\??",
                r"ìˆë‚˜ìš”\??",
            ]

            for pattern in remove_patterns:
                location_query = re.sub(
                    pattern, " ", location_query, flags=re.IGNORECASE
                )

            # "ì—ì„œ", "ì˜" ë“± ì¡°ì‚¬ ì œê±°
            location_query = re.sub(r"\s*(ì—ì„œ|ì—|ì˜|ì—ì„œì˜)\s*", " ", location_query)

            # ê³µë°± ì •ë¦¬
            location_query = " ".join(location_query.split()).strip()

            condition_text = "ì´ìƒ" if is_minimum else "ì´í•˜"
            print(
                f"[search_shelter_by_capacity] ìˆ˜ìš©ì¸ì›: {capacity_value}ëª… {condition_text}"
            )
            print(f"[search_shelter_by_capacity] ìœ„ì¹˜ í•„í„°: '{location_query}'")

            # ëª¨ë“  ëŒ€í”¼ì†Œ ê°€ì ¸ì˜¤ê¸°
            all_data = vectorstore.get(where={"type": "shelter"})
            shelters = []

            for metadata in all_data["metadatas"]:
                capacity = int(metadata.get("capacity", 0))

                # ìˆ˜ìš©ì¸ì› ì¡°ê±´ ì²´í¬ (ì´ìƒ vs ì´í•˜)
                if is_minimum:
                    if capacity < capacity_value:
                        continue
                else:
                    if capacity > capacity_value:
                        continue

                # ìœ„ì¹˜ ì¡°ê±´ ì²´í¬ (ìœ„ì¹˜ í‚¤ì›Œë“œê°€ ìˆìœ¼ë©´)
                if location_query:
                    facility_name = metadata.get("facility_name", "").lower()
                    address = metadata.get("address", "").lower()
                    shelter_type = metadata.get("shelter_type", "").lower()

                    # ì‹œì„¤ëª…, ì£¼ì†Œ, ìœ„ì¹˜ìœ í˜•ì—ì„œ ìœ„ì¹˜ í‚¤ì›Œë“œ ê²€ìƒ‰
                    search_text = f"{facility_name} {address} {shelter_type}"

                    # ìœ„ì¹˜ í‚¤ì›Œë“œì˜ ëª¨ë“  ë¶€ë¶„ì´ í¬í•¨ë˜ì–´ì•¼ ë§¤ì¹­
                    location_keywords = location_query.split()
                    if not all(
                        keyword in search_text
                        for keyword in location_keywords
                        if keyword
                    ):
                        continue

                shelters.append(
                    {
                        "name": metadata.get("facility_name", "N/A"),
                        "address": metadata.get("address", "N/A"),
                        "lat": float(metadata.get("lat", 0)),
                        "lon": float(metadata.get("lon", 0)),
                        "capacity": capacity,
                        "shelter_type": metadata.get("shelter_type", "N/A"),
                        "distance": 0,  # ìˆ˜ìš©ì¸ì› ê²€ìƒ‰ì€ ê±°ë¦¬ ì •ë³´ ì—†ìŒ
                    }
                )

            # ìˆ˜ìš©ì¸ì› ë‚´ë¦¼ì°¨ìˆœ ì •ë ¬
            shelters.sort(key=lambda x: x["capacity"], reverse=True)
            top_10 = shelters[:10]

            if not top_10:
                location_text = (
                    f"'{location_query}' ì§€ì—­ì—ì„œ " if location_query else ""
                )
                condition_text = "ì´ìƒ" if is_minimum else "ì´í•˜"
                return {
                    "text": f"{location_text}{capacity_value:,}ëª… {condition_text} ìˆ˜ìš© ê°€ëŠ¥í•œ ëŒ€í”¼ì†Œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.",
                    "structured_data": None,
                }

            location_text = f"**{location_query}** ì§€ì—­ " if location_query else ""
            condition_text = "ì´ìƒ" if is_minimum else "ì´í•˜"
            result = f"ğŸ“Š {location_text}**{capacity_value:,}ëª… {condition_text}** ìˆ˜ìš© ê°€ëŠ¥í•œ ëŒ€í”¼ì†Œ **{len(shelters)}ê³³** ì¤‘ ìƒìœ„ 10ê³³\n\n"
            for i, s in enumerate(top_10, 1):
                result += f"{i}. **{s['name']}** ({s['capacity']:,}ëª…)\n"
                result += f"   ğŸ“ {s['address']}\n"
                result += f"   ğŸ“ ìœ„ì¹˜: {s['shelter_type']}\n\n"

            # ì¤‘ì‹¬ ì¢Œí‘œ ê³„ì‚° (í‰ê· )
            avg_lat = (
                sum(s["lat"] for s in top_10 if s["lat"] != 0)
                / len([s for s in top_10 if s["lat"] != 0])
                if any(s["lat"] != 0 for s in top_10)
                else 0
            )
            avg_lon = (
                sum(s["lon"] for s in top_10 if s["lon"] != 0)
                / len([s for s in top_10 if s["lon"] != 0])
                if any(s["lon"] != 0 for s in top_10)
                else 0
            )

            structured_data = {
                "location": (
                    f"{location_query} {capacity_value:,}ëª… {condition_text}"
                    if location_query
                    else f"{capacity_value:,}ëª… {condition_text} ìˆ˜ìš© ê°€ëŠ¥"
                ),
                "coordinates": (avg_lat, avg_lon) if avg_lat != 0 else None,
                "shelters": top_10,
                "total_count": len(shelters),
            }

            return {"text": result.strip(), "structured_data": structured_data}

        except Exception as e:
            print(f"[ERROR] search_shelter_by_capacity: {e}")
            import traceback

            traceback.print_exc()
            return {"text": f"ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}", "structured_data": None}

    @tool
    def search_disaster_guideline(query: str) -> dict:
        """
        ì¬ë‚œ í–‰ë™ìš”ë ¹ì„ ê²€ìƒ‰í•©ë‹ˆë‹¤.

        Args:
            query: ì¬ë‚œ ìœ í˜• (ì˜ˆ: "ì§€ì§„", "í™”ì¬", "ì‚°ì‚¬íƒœ")

        Returns:
            dict: {"text": str, "structured_data": None} í˜•ì‹
        """
        try:
            # ì¿¼ë¦¬ ì¬ì •ì˜
            rewritten = query_rewrite_chain.invoke({"original_query": query})
            print(f"[search_disaster_guideline] ì¬ì •ì˜: {query} â†’ {rewritten}")

            if guideline_hybrid is None:
                return {
                    "text": "ê°€ì´ë“œë¼ì¸ ê²€ìƒ‰ ì‹œìŠ¤í…œì´ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.",
                    "structured_data": None,
                }

            # í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰
            results = guideline_hybrid.invoke(rewritten)

            if not results:
                return {
                    "text": f"'{query}' ê´€ë ¨ í–‰ë™ìš”ë ¹ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.",
                    "structured_data": None,
                }

            # ìƒìœ„ 3ê°œ ê²°ê³¼ í†µí•©
            combined = "\n\n".join([doc.page_content for doc in results[:3]])

            return {
                "text": f"ğŸš¨ **{query} í–‰ë™ìš”ë ¹**\n\n{combined}",
                "structured_data": None,  # í–‰ë™ìš”ë ¹ì€ ìœ„ì¹˜ ì •ë³´ ì—†ìŒ
            }

        except Exception as e:
            print(f"[ERROR] search_disaster_guideline: {e}")
            return {"text": f"ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}", "structured_data": None}

    @tool
    def answer_general_knowledge(query: str) -> dict:
        """
        ì¬ë‚œ ê´€ë ¨ ì¼ë°˜ ì§€ì‹ ì§ˆë¬¸ì— ë‹µë³€í•©ë‹ˆë‹¤. (ì •ì˜, ì›ì¸, íŠ¹ì§• ë“±)
        VectorDBì— ì—†ëŠ” ì •ë³´ëŠ” LLMì˜ ì‚¬ì „ í•™ìŠµ ì§€ì‹ì„ í™œìš©í•©ë‹ˆë‹¤.

        Args:
            query: ì¼ë°˜ ì§€ì‹ ì§ˆë¬¸ (ì˜ˆ: "ì§€ì§„ì´ ë­ì•¼", "ì“°ë‚˜ë¯¸ë€")

        Returns:
            dict: {"text": str, "structured_data": None} í˜•ì‹
        """
        try:
            print(f"[answer_general_knowledge] ì§ˆë¬¸: {query}")

            # LLMì—ê²Œ ì§ì ‘ ì§ˆë¬¸ (ì‚¬ì „ í•™ìŠµ ì§€ì‹ í™œìš©)
            prompt = f"""ë‹¹ì‹ ì€ ì¬ë‚œ ì•ˆì „ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
ë‹¤ìŒ ì§ˆë¬¸ì— ì •í™•í•˜ê³  ê°„ê²°í•˜ê²Œ ë‹µë³€í•˜ì„¸ìš”.

ì§ˆë¬¸: {query}

ë‹µë³€ í˜•ì‹:
- í•µì‹¬ ì •ì˜ë¥¼ 2-3ë¬¸ì¥ìœ¼ë¡œ ì„¤ëª…
- ì£¼ìš” íŠ¹ì§•ì´ë‚˜ ì›ì¸ì„ ë¶ˆë¦¿ í¬ì¸íŠ¸ë¡œ ì •ë¦¬
- ì „ë¬¸ ìš©ì–´ëŠ” ì‰½ê²Œ í’€ì–´ì„œ ì„¤ëª…
- ìµœëŒ€ 200ì ì´ë‚´ë¡œ ê°„ê²°í•˜ê²Œ"""

            response = llm_creative.invoke([HumanMessage(content=prompt)])

            return {
                "text": f"ğŸ’¡ **{query}**\n\n{response.content}",
                "structured_data": None,  # ì¼ë°˜ ì§€ì‹ì€ ìœ„ì¹˜ ì •ë³´ ì—†ìŒ
            }

        except Exception as e:
            print(f"[ERROR] answer_general_knowledge: {e}")
            return {
                "text": "ì£„ì†¡í•©ë‹ˆë‹¤. ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.",
                "structured_data": None,
            }

    @tool
    def search_shelter_by_name(query: str) -> dict:
        """
        íŠ¹ì • ëŒ€í”¼ì†Œì˜ ìƒì„¸ ì •ë³´ë¥¼ ì‹œì„¤ëª…ìœ¼ë¡œ ê²€ìƒ‰í•©ë‹ˆë‹¤.
        ìœ„ì¹˜ ì¡°ê±´ì´ ìˆìœ¼ë©´ í•´ë‹¹ ì§€ì—­ ë‚´ì—ì„œë§Œ ê²€ìƒ‰í•©ë‹ˆë‹¤.
        ìˆ˜ìš©ì¸ì›, ì£¼ì†Œ, ìœ„ì¹˜ ë“± í•´ë‹¹ ì‹œì„¤ì˜ ëª¨ë“  ì •ë³´ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
        ì§€ë„ í‘œì‹œìš© êµ¬ì¡°í™”ëœ ë°ì´í„°ë¥¼ í¬í•¨í•©ë‹ˆë‹¤.

        Args:
            query: ëŒ€í”¼ì†Œ ì‹œì„¤ëª… (ì˜ˆ: "ë™ëŒ€ë¬¸ë§¨ì…˜", "ì œì£¼ë„ ë™ì•„ì•„íŒŒíŠ¸", "ì„œìš¸ ë¡¯ë°ì›”ë“œ")

        Returns:
            dict: {"text": str, "structured_data": dict} í˜•ì‹

        Examples:
            - "ë™ëŒ€ë¬¸ë§¨ì…˜ ìˆ˜ìš©ì¸ì›" â†’ search_shelter_by_name("ë™ëŒ€ë¬¸ë§¨ì…˜")
            - "ì œì£¼ë„ ë™ì•„ì•„íŒŒíŠ¸ ì •ë³´" â†’ search_shelter_by_name("ì œì£¼ë„ ë™ì•„ì•„íŒŒíŠ¸")
        """
        try:
            print(f"[search_shelter_by_name] ê²€ìƒ‰ ì‹œì‘: '{query}'")

            # 1ë‹¨ê³„: ìœ„ì¹˜ì™€ ì‹œì„¤ëª… ë¶„ë¦¬
            original_query = query.strip().lower()

            # ì§€ì—­ í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸ (ì‹œ/ë„/êµ¬ ë‹¨ìœ„)
            location_keywords = [
                # ì œì£¼ ê´€ë ¨ (ê¸¸ì´ìˆœ ì •ë ¬ - ê¸´ ê²ƒë¶€í„°)
                "ì œì£¼íŠ¹ë³„ìì¹˜ë„",
                "ì œì£¼ë„",
                "ì œì£¼ì‹œ",
                "ì„œê·€í¬ì‹œ",
                "ì œì£¼",
                # ê´‘ì—­ì‹œ/ë„
                "ì„œìš¸íŠ¹ë³„ì‹œ",
                "ì„œìš¸",
                "ë¶€ì‚°ê´‘ì—­ì‹œ",
                "ë¶€ì‚°",
                "ëŒ€êµ¬ê´‘ì—­ì‹œ",
                "ëŒ€êµ¬",
                "ì¸ì²œê´‘ì—­ì‹œ",
                "ì¸ì²œ",
                "ê´‘ì£¼ê´‘ì—­ì‹œ",
                "ê´‘ì£¼",
                "ëŒ€ì „ê´‘ì—­ì‹œ",
                "ëŒ€ì „",
                "ìš¸ì‚°ê´‘ì—­ì‹œ",
                "ìš¸ì‚°",
                "ì„¸ì¢…íŠ¹ë³„ìì¹˜ì‹œ",
                "ì„¸ì¢…",
                "ê²½ê¸°ë„",
                "ê²½ê¸°",
                "ê°•ì›ë„",
                "ê°•ì›íŠ¹ë³„ìì¹˜ë„",
                "ê°•ì›",
                "ì¶©ì²­ë¶ë„",
                "ì¶©ë¶",
                "ì¶©ì²­ë‚¨ë„",
                "ì¶©ë‚¨",
                "ì „ë¼ë¶ë„",
                "ì „ë¶",
                "ì „ë¼ë‚¨ë„",
                "ì „ë‚¨",
                "ê²½ìƒë¶ë„",
                "ê²½ë¶",
                "ê²½ìƒë‚¨ë„",
                "ê²½ë‚¨",
                # ì„œìš¸ êµ¬
                "ê°•ë‚¨êµ¬",
                "ê°•ë™êµ¬",
                "ê°•ë¶êµ¬",
                "ê°•ì„œêµ¬",
                "ê´€ì•…êµ¬",
                "ê´‘ì§„êµ¬",
                "êµ¬ë¡œêµ¬",
                "ê¸ˆì²œêµ¬",
                "ë…¸ì›êµ¬",
                "ë„ë´‰êµ¬",
                "ë™ëŒ€ë¬¸êµ¬",
                "ë™ì‘êµ¬",
                "ë§ˆí¬êµ¬",
                "ì„œëŒ€ë¬¸êµ¬",
                "ì„œì´ˆêµ¬",
                "ì„±ë™êµ¬",
                "ì„±ë¶êµ¬",
                "ì†¡íŒŒêµ¬",
                "ì–‘ì²œêµ¬",
                "ì˜ë“±í¬êµ¬",
                "ìš©ì‚°êµ¬",
                "ì€í‰êµ¬",
                "ì¢…ë¡œêµ¬",
                "ì¤‘êµ¬",
                "ì¤‘ë‘êµ¬",
            ]

            # ìœ„ì¹˜ í‚¤ì›Œë“œ ì¶”ì¶œ (ê¸´ ê²ƒë¶€í„° ë§¤ì¹­ - ì´ë¯¸ ì •ë ¬ë¨)
            location_filter = None
            for loc in location_keywords:
                if loc in original_query:
                    location_filter = loc
                    print(f"[DEBUG] ìœ„ì¹˜ í‚¤ì›Œë“œ '{loc}' ë§¤ì¹­ë¨")
                    break

            print(f"[DEBUG] ìµœì¢… location_filter: '{location_filter}'")

            # 2ë‹¨ê³„: ê²€ìƒ‰ì–´ ì •ì œ (ë¶ˆí•„ìš”í•œ ë‹¨ì–´ ì œê±°)
            search_term = original_query

            # ìœ„ì¹˜ í‚¤ì›Œë“œ ë¨¼ì € ì œê±° (ì‹œì„¤ëª…ë§Œ ë‚¨ê¹€)
            if location_filter:
                search_term = search_term.replace(location_filter, " ")

            # ë¶ˆí•„ìš”í•œ ë‹¨ì–´ ì œê±°
            remove_words = [
                "ëŒ€í”¼ì†Œ",
                "ìˆ˜ìš©ì¸ì›",
                "ìµœëŒ€ìˆ˜ìš©ì¸ì›",
                "ëª‡ëª…",
                "ì •ë³´",
                "ì•Œë ¤ì¤˜",
                "ì•Œë ¤",
                "ì˜",
                "ì´",
                "ê°€",
                "ì€",
                "ëŠ”",
                "?",
                "!",
                "ë¥¼",
                "ì„",
                "ë„",
                "ì‹œ",
                "êµ°",
                "êµ¬",
            ]  # í–‰ì •êµ¬ì—­ ë‹¨ìœ„ë„ ì œê±°

            for word in remove_words:
                search_term = search_term.replace(word, " ")

            search_term = (
                " ".join(search_term.split()).strip().lower()
            )  # ì†Œë¬¸ì ë³€í™˜ ì¶”ê°€

            print(f"[search_shelter_by_name] ì •ì œëœ ê²€ìƒ‰ì–´: '{search_term}'")
            print(f"[search_shelter_by_name] ìœ„ì¹˜ í•„í„°: '{location_filter}'")

            # VectorStoreì—ì„œ shelter íƒ€ì… ë¬¸ì„œ ê°€ì ¸ì˜¤ê¸°
            all_data = vectorstore.get(where={"type": "shelter"})

            # 3ë‹¨ê³„: ì‹œì„¤ëª… ë§¤ì¹­ (ë¶€ë¶„ ì¼ì¹˜)
            matches = []
            match_attempt = 0
            for metadata in all_data["metadatas"]:
                facility_name = metadata.get("facility_name", "")
                facility_lower = facility_name.lower()
                address = metadata.get("address", "").lower()

                # ì‹œì„¤ëª… ë§¤ì¹­ (ì–‘ë°©í–¥ ë¶€ë¶„ ì¼ì¹˜)
                if search_term in facility_lower or facility_lower in search_term:
                    match_attempt += 1

                    # ìœ„ì¹˜ í•„í„°ê°€ ìˆìœ¼ë©´ ì£¼ì†Œë„ ì²´í¬
                    if location_filter:
                        filter_lower = location_filter.lower()

                        # ìœ ì—°í•œ ìœ„ì¹˜ ë§¤ì¹­ - í–‰ì •êµ¬ì—­ ë‹¨ìœ„ ì œê±° (ê¸´ ê²ƒë¶€í„° ìˆœì„œëŒ€ë¡œ)
                        filter_core = (
                            filter_lower.replace(
                                "íŠ¹ë³„ìì¹˜ë„", ""
                            )  # 'ì œì£¼íŠ¹ë³„ìì¹˜ë„' â†’ 'ì œì£¼'
                            .replace("íŠ¹ë³„ìì¹˜ì‹œ", "")  # 'ì„¸ì¢…íŠ¹ë³„ìì¹˜ì‹œ' â†’ 'ì„¸ì¢…'
                            .replace("íŠ¹ë³„ì‹œ", "")  # 'ì„œìš¸íŠ¹ë³„ì‹œ' â†’ 'ì„œìš¸'
                            .replace("ê´‘ì—­ì‹œ", "")  # 'ë¶€ì‚°ê´‘ì—­ì‹œ' â†’ 'ë¶€ì‚°'
                            .replace("ë„", "")  # 'ê²½ê¸°ë„' â†’ 'ê²½ê¸°', 'ì œì£¼ë„' â†’ 'ì œì£¼'
                            .replace("ì‹œ", "")
                            .replace("êµ°", "")
                            .replace("êµ¬", "")
                            .strip()
                        )

                        if match_attempt <= 3:
                            print(
                                f"[DEBUG] ì‹œì„¤ëª… ë§¤ì¹­: '{facility_name}', ì£¼ì†Œ: '{address[:30]}...', filter_lower: '{filter_lower}', filter_core: '{filter_core}', í¬í•¨ì—¬ë¶€: {filter_core in address}"
                            )

                        if filter_core not in address:
                            continue

                    matches.append(
                        {
                            "name": facility_name,
                            "address": metadata.get("address", "N/A"),
                            "lat": float(metadata.get("lat", 0)),
                            "lon": float(metadata.get("lon", 0)),
                            "capacity": int(metadata.get("capacity", 0)),
                            "shelter_type": metadata.get("shelter_type", "N/A"),
                            "facility_type": metadata.get("facility_type", "N/A"),
                            "operating_status": metadata.get("operating_status", "N/A"),
                            "distance": 0,  # ì‹œì„¤ëª… ê²€ìƒ‰ì€ ê±°ë¦¬ ì •ë³´ ì—†ìŒ
                        }
                    )
                    print(
                        f"[search_shelter_by_name] ë§¤ì¹­ë¨: {facility_name} ({metadata.get('address', 'N/A')})"
                    )

            print(f"[DEBUG] ì´ ë§¤ì¹­ëœ ëŒ€í”¼ì†Œ ê°œìˆ˜: {len(matches)}")

            if not matches:
                location_text = f"{location_filter} " if location_filter else ""
                return {
                    "text": f"âŒ '{location_text}{search_term}' ì‹œì„¤ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\nì‹œì„¤ëª…ì„ ì •í™•íˆ ì…ë ¥í•´ì£¼ì„¸ìš”.",
                    "structured_data": None,
                }

            # ê²°ê³¼ ë°˜í™˜
            if len(matches) == 1:
                m = matches[0]
                text = f"""ğŸ“ **{m['name']}**

    âœ… **ìµœëŒ€ ìˆ˜ìš©ì¸ì›: {m['capacity']:,}ëª…**
    ğŸ“ ì£¼ì†Œ: {m['address']}
    ğŸ“ ìœ„ì¹˜: {m['shelter_type']}
    ğŸ“ ì‹œì„¤ ìœ í˜•: {m['facility_type']}
    ğŸ“ ìš´ì˜ ìƒíƒœ: {m['operating_status']}"""

                # êµ¬ì¡°í™”ëœ ë°ì´í„° (ì§€ë„ í‘œì‹œìš©)
                structured_data = {
                    "location": m["name"],
                    "coordinates": (m["lat"], m["lon"]) if m["lat"] != 0 else None,
                    "shelters": [m],
                    "total_count": 1,
                }

                return {"text": text, "structured_data": structured_data}

            else:
                # ì—¬ëŸ¬ ê°œ ë°œê²¬ ì‹œ
                print(f"[DEBUG] ì—¬ëŸ¬ ê°œ ë°œê²¬ ë¶„ê¸° ì§„ì…: {len(matches)}ê°œ")
                text = (
                    f"ğŸ“ **'{search_term}'** ê´€ë ¨ ëŒ€í”¼ì†Œ **{len(matches)}ê³³** ë°œê²¬\n\n"
                )
                for i, m in enumerate(matches[:5], 1):  # ìƒìœ„ 5ê°œë§Œ
                    text += f"{i}. **{m['name']}**\n"
                    text += f"   âœ… ìˆ˜ìš©ì¸ì›: **{m['capacity']:,}ëª…**\n"
                    text += f"   ğŸ“ ì£¼ì†Œ: {m['address']}\n"
                    text += f"   ğŸ“ ìœ„ì¹˜: {m['shelter_type']}\n\n"

                if len(matches) > 5:
                    text += f"ğŸ’¡ ì™¸ {len(matches) - 5}ê³³ ë” ìˆìŠµë‹ˆë‹¤."

                # ì¤‘ì‹¬ ì¢Œí‘œ ê³„ì‚° (í‰ê· )
                avg_lat = (
                    sum(s["lat"] for s in matches if s["lat"] != 0)
                    / len([s for s in matches if s["lat"] != 0])
                    if any(s["lat"] != 0 for s in matches)
                    else 0
                )
                avg_lon = (
                    sum(s["lon"] for s in matches if s["lon"] != 0)
                    / len([s for s in matches if s["lon"] != 0])
                    if any(s["lon"] != 0 for s in matches)
                    else 0
                )

                structured_data = {
                    "location": search_term,
                    "coordinates": (avg_lat, avg_lon) if avg_lat != 0 else None,
                    "shelters": matches[:5],
                    "total_count": len(matches),
                }

                return {"text": text.strip(), "structured_data": structured_data}

        except Exception as e:
            print(f"[ERROR] search_shelter_by_name: {e}")
            import traceback

            traceback.print_exc()
            return {"text": f"âŒ ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}", "structured_data": None}

    @tool
    def search_location_with_disaster(query: str) -> dict:
        """
        íŠ¹ì • ìœ„ì¹˜ì—ì„œ ì¬ë‚œ ë°œìƒ ì‹œ ëŒ€í”¼ì†Œì™€ í–‰ë™ìš”ë ¹ì„ í•¨ê»˜ ì œê³µí•©ë‹ˆë‹¤.
        ìœ„ì¹˜ ê¸°ë°˜ ëŒ€í”¼ì†Œ ê²€ìƒ‰ + ì¬ë‚œ í–‰ë™ìš”ë ¹ì„ í†µí•©í•˜ì—¬ ë°˜í™˜í•©ë‹ˆë‹¤.
        ì§€ë„ í‘œì‹œìš© êµ¬ì¡°í™”ëœ ë°ì´í„°ë¥¼ í¬í•¨í•©ë‹ˆë‹¤.

        Args:
            query: ìœ„ì¹˜ + ì¬ë‚œ ìƒí™© (ì˜ˆ: "ì„¤ì•…ì‚° ê·¼ì²˜ ì‚°ì‚¬íƒœ", "ê°•ë‚¨ì—­ì—ì„œ ì§€ì§„")

        Returns:
            dict: {"text": str, "structured_data": dict} í˜•ì‹

        Examples:
            - "ì„¤ì•…ì‚° ê·¼ì²˜ì¸ë° ì‚°ì‚¬íƒœ ë°œìƒ ì‹œ" â†’ ì„¤ì•…ì‚° ëŒ€í”¼ì†Œ + ì‚°ì‚¬íƒœ í–‰ë™ìš”ë ¹
            - "ëª…ë™ì—ì„œ ì§€ì§„ ë‚˜ë©´" â†’ ëª…ë™ ëŒ€í”¼ì†Œ + ì§€ì§„ í–‰ë™ìš”ë ¹
        """
        try:
            print(f"[search_location_with_disaster] ë³µí•© ì§ˆë¬¸ ì²˜ë¦¬: {query}")

            # 1ë‹¨ê³„: ì¬ë‚œ ìœ í˜• ê°ì§€
            disaster_keywords = [
                "ì§€ì§„", "í™ìˆ˜", "íƒœí’", "í™”ì¬", "í­ë°œ", "ì‚°ì‚¬íƒœ", 
                "ì“°ë‚˜ë¯¸", "í™”ì‚°", "ë°©ì‚¬ëŠ¥", "ê°€ìŠ¤", "ë¶•ê´´", "í…ŒëŸ¬"
            ]

            detected_disaster = None
            location_query = query

            for keyword in disaster_keywords:
                if keyword in query:
                    detected_disaster = keyword
                    location_query = location_query.replace(keyword, "")
                    break

            # "ë°œìƒ", "ë‚˜ë©´", "ë‚¬ì„ ë•Œ" ë“± ì œê±°
            for word in ["ë°œìƒ", "ë°œìƒí•˜ë©´", "ë°œìƒ ì‹œ", "ë‚¬ì„ ë•Œ", "ë‚˜ë©´", "ë•Œ", "ê·¼ì²˜ì¸ë°", "ì—ì„œ", "ì–´ë–»ê²Œ", "ëŒ€ì²˜", "í–‰ë™ìš”ë ¹"]:
                location_query = location_query.replace(word, "")

            location_query = location_query.strip()
            
            if not detected_disaster:
                return {
                    "text": "ì¬ë‚œ ìœ í˜•ì„ íŒŒì•…í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì˜ˆ: 'ì„¤ì•…ì‚° ì‚°ì‚¬íƒœ', 'ê°•ë‚¨ì—­ ì§€ì§„'",
                    "structured_data": None,
                }

            print(f"[search_location_with_disaster] ìœ„ì¹˜: '{location_query}', ì¬ë‚œ: '{detected_disaster}'")

            # 2ë‹¨ê³„: ì§ˆë¬¸ ì¬ì •ì˜ë¡œ ìœ„ì¹˜ ìœ í˜• íŒë‹¨ (search_shelter_by_locationê³¼ ë™ì¼)
            rewritten = query_rewrite_chain.invoke({"original_query": location_query})
            
            kakao_query = location_query
            location_type = "specific"
            
            try:
                import json
                parsed = json.loads(rewritten)
                kakao_query = parsed.get("kakao", location_query)
                vector_query = parsed.get("vector", location_query)
                location_type = parsed.get("location_type", "specific")
                
                print(f"[search_location_with_disaster] ìœ„ì¹˜ ìœ í˜•: {location_type}")
                print(f"[search_location_with_disaster] ì¹´ì¹´ì˜¤ìš©: '{kakao_query}'")
                print(f"[search_location_with_disaster] Vectorìš©: '{vector_query}'")
                
            except:
                # JSON íŒŒì‹± ì‹¤íŒ¨ ì‹œ ê¸°ì¡´ ì •ì œ ë¡œì§
                remove_words = ["ê·¼ì²˜", "ì£¼ë³€", "ì¸ê·¼", "ëŒ€í”¼ì†Œ", "í”¼ë‚œì†Œ", "í”¼ë‚œì²˜"]
                for word in remove_words:
                    kakao_query = kakao_query.replace(word, "")
                kakao_query = " ".join(kakao_query.split()).strip()

            print(f"[search_location_with_disaster] ìµœì¢… ì¹´ì¹´ì˜¤ ê²€ìƒ‰ì–´: '{kakao_query}' ({location_type})")

            # 3ë‹¨ê³„: ì¹´ì¹´ì˜¤ APIë¡œ ì¢Œí‘œ ê²€ìƒ‰ (search_shelter_by_locationê³¼ ë™ì¼)
            kakao_api_key = os.getenv("KAKAO_REST_API_KEY")
            if not kakao_api_key:
                return {"text": "ì¹´ì¹´ì˜¤ API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.", "structured_data": None}

            headers = {"Authorization": f"KakaoAK {kakao_api_key}"}
            url = "https://dapi.kakao.com/v2/local/search/keyword.json"
            params = {"query": kakao_query}

            try:
                response = requests.get(url, headers=headers, params=params)
                data = response.json()

                if not data.get("documents"):
                    return {
                        "text": f"'{kakao_query}' ìœ„ì¹˜ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.",
                        "structured_data": None,
                    }

                place = data["documents"][0]
                user_lat = float(place["y"])
                user_lon = float(place["x"])
                place_name = place["place_name"]
                
                location_desc = f"{place_name} ({location_type})"
                print(f"[search_location_with_disaster] ì¥ì†Œ í™•ì¸: {location_desc} ({user_lat}, {user_lon})")

            except Exception as e:
                print(f"[search_location_with_disaster] ì¹´ì¹´ì˜¤ API ì˜¤ë¥˜: {e}")
                return {
                    "text": f"ì¹´ì¹´ì˜¤ API í˜¸ì¶œ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}",
                    "structured_data": None,
                }

            # 4ë‹¨ê³„: ê·¼ì²˜ ëŒ€í”¼ì†Œ ê²€ìƒ‰ (ê±°ë¦¬ ê³„ì‚°)
            def haversine(lat1, lon1, lat2, lon2):
                from math import radians, sin, cos, sqrt, atan2
                R = 6371
                dlat = radians(lat2 - lat1)
                dlon = radians(lon2 - lon1)
                a = (
                    sin(dlat / 2) ** 2
                    + cos(radians(lat1)) * cos(radians(lat2)) * sin(dlon / 2) ** 2
                )
                c = 2 * atan2(sqrt(a), sqrt(1 - a))
                return R * c

            all_data = vectorstore.get(where={"type": "shelter"})
            shelters = []

            for metadata in all_data["metadatas"]:
                try:
                    lat = float(metadata.get("lat", 0))
                    lon = float(metadata.get("lon", 0))
                    if lat == 0 or lon == 0:
                        continue

                    distance = haversine(user_lat, user_lon, lat, lon)
                    shelters.append(
                        {
                            "name": metadata.get("facility_name", "N/A"),
                            "address": metadata.get("address", "N/A"),
                            "lat": lat,
                            "lon": lon,
                            "distance": distance,
                            "capacity": int(metadata.get("capacity", 0)),
                            "shelter_type": metadata.get("shelter_type", "N/A"),
                            "facility_type": metadata.get("facility_type", "N/A"),
                        }
                    )
                except Exception:
                    continue

            shelters.sort(key=lambda x: x["distance"])
            top_3 = shelters[:3]  # ê°€ì¥ ê°€ê¹Œìš´ 3ê³³ë§Œ

            if not top_3:
                return {
                    "text": f"'{place_name}' ê·¼ì²˜ì— ëŒ€í”¼ì†Œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.",
                    "structured_data": None,
                }

            # 5ë‹¨ê³„: ì¬ë‚œ í–‰ë™ìš”ë ¹ ê²€ìƒ‰
            guideline_text = ""
            if guideline_hybrid:
                try:
                    guideline_results = guideline_hybrid.invoke(detected_disaster)
                    if guideline_results:
                        # ìƒìœ„ 2ê°œ ê²°ê³¼ë§Œ ì‚¬ìš© (ê°„ê²°í•˜ê²Œ)
                        guideline_text = "\n\n".join(
                            [doc.page_content for doc in guideline_results[:2]]
                        )
                except Exception as e:
                    print(f"[search_location_with_disaster] ê°€ì´ë“œë¼ì¸ ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
                    guideline_text = f"{detected_disaster} ê´€ë ¨ í–‰ë™ìš”ë ¹ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

            # 6ë‹¨ê³„: í†µí•© ê²°ê³¼ ìƒì„±
            location_text = "ì§€ì—­" if location_type == "region" else "ìœ„ì¹˜"
            result = f"""ğŸš¨ **{place_name} {location_text} ê¸°ì¤€ {detected_disaster} ë°œìƒ ì‹œ ëŒ€ì‘ ê°€ì´ë“œ**

    ğŸ“ **ê°€ì¥ ê°€ê¹Œìš´ ëŒ€í”¼ì†Œ {len(top_3)}ê³³**

    """

            for i, s in enumerate(top_3, 1):
                result += f"{i}. **{s['name']}** ({s['distance']:.2f}km)\n"
                result += f"   ğŸ“ {s['address']}\n"
                result += f"   ğŸ“ ìœ„ì¹˜: {s['shelter_type']} | ìˆ˜ìš©: {s['capacity']:,}ëª…\n\n"

            result += f"""

    ğŸš¨ **{detected_disaster} í–‰ë™ìš”ë ¹**

    {guideline_text}

    ğŸ’¡ **ì¦‰ì‹œ í–‰ë™ ì²´í¬ë¦¬ìŠ¤íŠ¸**
    âœ… ê°€ì¥ ê°€ê¹Œìš´ ëŒ€í”¼ì†Œë¡œ ì´ë™
    âœ… ìœ„ í–‰ë™ìš”ë ¹ì„ ìˆ™ì§€í•˜ê³  ì¹¨ì°©í•˜ê²Œ ëŒ€ì‘
    âœ… 119 ì‹ ê³  (í•„ìš” ì‹œ)
    """

            # êµ¬ì¡°í™”ëœ ë°ì´í„° (ì§€ë„ í‘œì‹œìš©)
            structured_data = {
                "location": place_name,
                "location_type": location_type,  # NEW
                "user_coordinates": [user_lat, user_lon],  # ì‚¬ìš©ì ìœ„ì¹˜ (ê¸¸ì°¾ê¸°ìš©)
                "coordinates": [user_lat, user_lon],
                "shelters": top_3,
                "total_count": len(all_data["metadatas"]),
            }

            return {"text": result.strip(), "structured_data": structured_data}

        except Exception as e:
            print(f"[ERROR] search_location_with_disaster: {e}")
            import traceback
            traceback.print_exc()
            return {
                "text": f"ë³µí•© ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}",
                "structured_data": None,
            }

    # 6. Tools ë¦¬ìŠ¤íŠ¸
    tools = [
        search_shelter_by_location,
        count_shelters,
        search_shelter_by_capacity,
        search_disaster_guideline,
        answer_general_knowledge,
        search_shelter_by_name,
        search_location_with_disaster,
    ]

    # 7. LLMì— Tools ë°”ì¸ë”©
    llm_with_tools = llm.bind_tools(tools)

    # 8. State ì •ì˜
    class AgentState(TypedDict):
        messages: Annotated[list[BaseMessage], add_messages]
        intent: str
        rewritten_query: str
        structured_data: Optional[dict]  # ì§€ë„ í‘œì‹œìš© êµ¬ì¡°í™”ëœ ë°ì´í„°

    # 9. ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸
    SYSTEM_PROMPT = """ë‹¹ì‹ ì€ ëŒ€í•œë¯¼êµ­ì˜ ì¬ë‚œ ì•ˆì „ ì „ë¬¸ AI ë„ìš°ë¯¸ì…ë‹ˆë‹¤.

**í•µì‹¬ ì›ì¹™**:
1. **ì •í™•ì„± ìš°ì„ **: ì œê³µëœ ë„êµ¬ ê²°ê³¼ë§Œ ì‚¬ìš©í•˜ê³ , ì—†ëŠ” ì •ë³´ëŠ” ì§€ì–´ë‚´ì§€ ë§ˆì„¸ìš”
2. **ì˜ë„ íŒŒì•…**: ì‚¬ìš©ì ì§ˆë¬¸ì˜ ì˜ë„ë¥¼ ì •í™•íˆ ë¶„ë¥˜í•˜ê³  ì ì ˆí•œ ë„êµ¬ë¥¼ ì„ íƒí•˜ì„¸ìš”
3. **ë³µí•© ì§ˆë¬¸ ì²˜ë¦¬**: ì—¬ëŸ¬ ì˜ë„ê°€ ì„ì¸ ì§ˆë¬¸ì€ ìˆœì°¨ì ìœ¼ë¡œ ì²˜ë¦¬í•˜ì„¸ìš”

**ë„êµ¬ ì„ íƒ ê°€ì´ë“œ**:
- ìœ„ì¹˜ + ì¬ë‚œ ë³µí•© ì§ˆë¬¸ â†’ search_location_with_disaster
   - "ì„¤ì•…ì‚° ê·¼ì²˜ì¸ë° ì‚°ì‚¬íƒœ ë°œìƒ ì‹œ" â†’ search_location_with_disaster("ì„¤ì•…ì‚° ì‚°ì‚¬íƒœ")
   - "ê°•ë‚¨ì—­ì—ì„œ ì§€ì§„ ë‚˜ë©´" â†’ search_location_with_disaster("ê°•ë‚¨ì—­ ì§€ì§„")
   - "ëª…ë™ í™”ì¬ ë‚¬ì„ ë•Œ" â†’ search_location_with_disaster("ëª…ë™ í™”ì¬")
   
- íŠ¹ì • ì‹œì„¤ëª…ì´ í¬í•¨ëœ ì§ˆë¬¸ â†’ search_shelter_by_name
   - "ë™ëŒ€ë¬¸ë§¨ì…˜ ìˆ˜ìš©ì¸ì›" â†’ search_shelter_by_name("ë™ëŒ€ë¬¸ë§¨ì…˜")
   - "ì„œìš¸ì—­ ëŒ€í”¼ì†Œ ì •ë³´" â†’ search_shelter_by_name("ì„œìš¸ì—­")
   
- "ê·¼ì²˜", "ì£¼ë³€" í‚¤ì›Œë“œë§Œ â†’ search_shelter_by_location
   - "ê°•ë‚¨ì—­ ê·¼ì²˜ ëŒ€í”¼ì†Œ" â†’ search_shelter_by_location("ê°•ë‚¨ì—­")
   - "ëª…ë™ ì£¼ë³€ í”¼ë‚œì†Œ" â†’ search_shelter_by_location("ëª…ë™")

- "Xëª… ì´ìƒ/ì´í•˜" ì¡°ê±´ â†’ search_shelter_by_capacity
   - "1000ëª… ì´ìƒ ìˆ˜ìš© ê°€ëŠ¥í•œ ëŒ€í”¼ì†Œ" â†’ search_shelter_by_capacity("1000ëª… ì´ìƒ")

- "ê°œìˆ˜", "ëª‡ ê°œ" â†’ count_shelters
   - "ì„œìš¸ ì§€í•˜ ëŒ€í”¼ì†Œ ëª‡ ê°œ?" â†’ count_shelters("ì„œìš¸ ì§€í•˜")

- ì¬ë‚œ í–‰ë™ìš”ë ¹ë§Œ â†’ search_disaster_guideline
   - "ì§€ì§„ ë°œìƒ ì‹œ í–‰ë™ìš”ë ¹" â†’ search_disaster_guideline("ì§€ì§„")
   - (ìœ„ì¹˜ ì •ë³´ ì—†ì´ í–‰ë™ìš”ë ¹ë§Œ í•„ìš”í•œ ê²½ìš°)

- ì¬ë‚œ ì¼ë°˜ ì§€ì‹ â†’ answer_general_knowledge
   - "ì§€ì§„ì´ ë­ì•¼?" â†’ answer_general_knowledge("ì§€ì§„ì´ ë­ì•¼")

**ì¤‘ìš” íŒë‹¨ ê¸°ì¤€**:
- ì§ˆë¬¸ì— "ìœ„ì¹˜ + ì¬ë‚œ"ì´ í•¨ê»˜ ìˆìœ¼ë©´ â†’ search_location_with_disaster
- ì§ˆë¬¸ì— "ì‹œì„¤ëª… + ì •ë³´ ìš”ì²­"ì´ ìˆìœ¼ë©´ â†’ search_shelter_by_name
- ì§ˆë¬¸ì— "ìœ„ì¹˜ + ê·¼ì²˜/ì£¼ë³€"ë§Œ ìˆìœ¼ë©´ â†’ search_shelter_by_location

**ì‘ë‹µ í˜•ì‹**:
- êµ¬ì²´ì ì´ê³  ì‹¤ìš©ì ì¸ ì •ë³´ ì œê³µ
- ì¤‘ìš” ì •ë³´ëŠ” **ë³¼ë“œì²´** ê°•ì¡°
- ìˆ«ìëŠ” ì‰¼í‘œ êµ¬ë¶„ (1,000ëª…)
- ì´ëª¨ì§€ ì ì ˆíˆ í™œìš© (ğŸ“ğŸš¨ğŸ’¡ğŸ“Š)
"""

    # 10. ë…¸ë“œ í•¨ìˆ˜ë“¤
    def intent_classifier_node(state: AgentState):
        """ì˜ë„ ë¶„ë¥˜ ë…¸ë“œ (LLMë§Œ ì‚¬ìš©)"""
        start_time = time.time()
        messages = state["messages"]
        last_message = messages[-1].content

        print(f"\n[ì˜ë„ë¶„ë¥˜ ë…¸ë“œ] ì…ë ¥: {last_message}")

        try:
            # LLM ê¸°ë°˜ ì˜ë„ ë¶„ë¥˜
            intent_result = intent_chain.invoke({"query": last_message})
            intent_data = json.loads(intent_result)
            intent = intent_data["intent"]

            elapsed = time.time() - start_time
            print(f"â±ï¸ [ì˜ë„ë¶„ë¥˜ ì‹œê°„] {elapsed:.3f}ì´ˆ")
            print(f"[ì˜ë„ë¶„ë¥˜ ë…¸ë“œ] ê²°ê³¼: {intent} (ì‹ ë¢°ë„: {intent_data.get('confidence', 0)})")

            return {"intent": intent}

        except Exception as e:
            elapsed = time.time() - start_time
            print(f"â±ï¸ [ì˜ë„ë¶„ë¥˜ ì‹œê°„ (ì‹¤íŒ¨)] {elapsed:.3f}ì´ˆ")
            print(f"[ì˜ë„ë¶„ë¥˜ ë…¸ë“œ] ì˜¤ë¥˜: {e}, ê¸°ë³¸ê°’ ì‚¬ìš©")
            return {"intent": "general_chat"}


    def query_rewrite_node(state: AgentState):
        """ì§ˆë¬¸ ì¬ì •ì˜ ë…¸ë“œ (ì‹œê°„ ì¸¡ì •)"""
        start_time = time.time()
        messages = state["messages"]
        last_message = messages[-1].content
        intent = state.get("intent", "")

        if intent in ["general_chat", "general_knowledge"]:
            return {"rewritten_query": last_message}

        print(f"\n[ì§ˆë¬¸ì¬ì •ì˜ ë…¸ë“œ] ì…ë ¥: {last_message}")

        try:
            rewritten = query_rewrite_chain.invoke({"original_query": last_message})
            
            # JSON íŒŒì‹± ì‹œë„
            try:
                import json
                parsed = json.loads(rewritten)
                kakao_query = parsed.get("kakao", last_message)
                vector_query = parsed.get("vector", last_message)
                
                print(f"[ì§ˆë¬¸ì¬ì •ì˜] ì¹´ì¹´ì˜¤ìš©: {kakao_query}")
                print(f"[ì§ˆë¬¸ì¬ì •ì˜] Vectorìš©: {vector_query}")
                
                # Stateì— ë‘ ì¿¼ë¦¬ ëª¨ë‘ ì €ì¥
                return {
                    "rewritten_query": vector_query,  # ê¸°ë³¸ê°’ (ê¸°ì¡´ ë¡œì§ ìœ ì§€)
                    "kakao_query": kakao_query,       # ì¹´ì¹´ì˜¤ ì „ìš© (NEW)
                }
            except (json.JSONDecodeError, KeyError):
                # JSON íŒŒì‹± ì‹¤íŒ¨ ì‹œ ê¸°ì¡´ ë°©ì‹ ì‚¬ìš©
                print(f"[ì§ˆë¬¸ì¬ì •ì˜] ë‹¨ì¼ ì¿¼ë¦¬: {rewritten}")
                return {"rewritten_query": rewritten}
            
        except Exception as e:
            elapsed = time.time() - start_time
            print(f"â±ï¸ [ì§ˆë¬¸ì¬ì •ì˜ ì‹œê°„ (ì‹¤íŒ¨)] {elapsed:.3f}ì´ˆ")
            print(f"[ì§ˆë¬¸ì¬ì •ì˜ ë…¸ë“œ] ì˜¤ë¥˜: {e}")
            return {"rewritten_query": last_message}


    def agent_node(state: AgentState):
        """ì—ì´ì „íŠ¸ ì¶”ë¡  ë…¸ë“œ (ì‹œê°„ ì¸¡ì •)"""
        start_time = time.time()
        messages = state["messages"]
        intent = state.get("intent", "")

        print(f"\n[ì—ì´ì „íŠ¸ ë…¸ë“œ] ì˜ë„: {intent}")

        if not any(isinstance(m, SystemMessage) for m in messages):
            messages = [SystemMessage(content=SYSTEM_PROMPT)] + messages

        response = llm_with_tools.invoke(messages)
        
        elapsed = time.time() - start_time
        print(f"â±ï¸ [LLM í˜¸ì¶œ ì‹œê°„] {elapsed:.3f}ì´ˆ")

        return {"messages": [response]}


    def tools_node_with_structured_data(state: AgentState):
        """ë„êµ¬ ì‹¤í–‰ ë…¸ë“œ (ì‹œê°„ ì¸¡ì •)"""
        start_time = time.time()
        from langgraph.prebuilt import ToolNode

        tool_node = ToolNode(tools)
        result = tool_node.invoke(state)

        # ë„êµ¬ ê²°ê³¼ì—ì„œ structured_data ì¶”ì¶œ
        messages = result.get("messages", [])
        structured_data = None

        for message in messages:
            if hasattr(message, "content"):
                content = message.content

                # contentê°€ ë¬¸ìì—´ì¸ ê²½ìš° JSON íŒŒì‹± ì‹œë„
                if isinstance(content, str):
                    try:
                        import json

                        parsed = json.loads(content)
                        if isinstance(parsed, dict) and "structured_data" in parsed:
                            # Noneì´ ì•„ë‹Œ structured_dataë¥¼ ìš°ì„ ì ìœ¼ë¡œ ì‚¬ìš©
                            if parsed["structured_data"] is not None:
                                structured_data = parsed["structured_data"]
                                print(
                                    f"[tools_node] structured_data ì¶”ì¶œ ì™„ë£Œ (JSON): True"
                                )
                            message.content = parsed.get("text", content)
                    except (json.JSONDecodeError, TypeError):
                        pass

                # contentê°€ dictì¸ ê²½ìš° ì§ì ‘ ì²˜ë¦¬
                elif isinstance(content, dict) and "structured_data" in content:
                    # Noneì´ ì•„ë‹Œ structured_dataë¥¼ ìš°ì„ ì ìœ¼ë¡œ ì‚¬ìš©
                    if content["structured_data"] is not None:
                        structured_data = content["structured_data"]
                        print(f"[tools_node] structured_data ì¶”ì¶œ ì™„ë£Œ (dict): True")
                    message.content = content.get("text", str(content))

        elapsed = time.time() - start_time
        print(f"â±ï¸ [ë„êµ¬ ì‹¤í–‰ ì‹œê°„] {elapsed:.3f}ì´ˆ")

        return {"messages": messages, "structured_data": structured_data}

    def should_continue(state: AgentState):
        """ë„êµ¬ ì‹¤í–‰ í•„ìš” ì—¬ë¶€ íŒë‹¨"""
        messages = state["messages"]
        last_message = messages[-1]

        # ë„êµ¬ í˜¸ì¶œì´ ìˆìœ¼ë©´ ë„êµ¬ ì‹¤í–‰
        if hasattr(last_message, "tool_calls") and last_message.tool_calls:
            return "tools"

        # ì—†ìœ¼ë©´ ì¢…ë£Œ
        return END

    def should_continue_after_tools(state: AgentState):
        """ë„êµ¬ ì‹¤í–‰ í›„ ì¶”ê°€ ì²˜ë¦¬ í•„ìš” ì—¬ë¶€ íŒë‹¨ (NEW)"""
        messages = state["messages"]
        last_message = messages[-1]
        
        # ë„êµ¬ ì‹¤í–‰ ê²°ê³¼ê°€ ìˆê³ , structured_dataê°€ ìˆìœ¼ë©´ ë°”ë¡œ ì¢…ë£Œ
        if state.get("structured_data") is not None:
            print("[ìµœì í™”] structured_data ì¡´ì¬ â†’ ì¦‰ì‹œ ì¢…ë£Œ")
            return END
        
        # ë„êµ¬ ì‹¤í–‰ ê²°ê³¼ê°€ í…ìŠ¤íŠ¸ë¡œë§Œ ìˆì–´ë„ ì¢…ë£Œ
        if hasattr(last_message, "content") and len(str(last_message.content)) > 50:
            print("[ìµœì í™”] ì¶©ë¶„í•œ ë‹µë³€ ì¡´ì¬ â†’ ì¦‰ì‹œ ì¢…ë£Œ")
            return END
        
        # ì¶”ê°€ ë„êµ¬ í˜¸ì¶œì´ í•„ìš”í•œ ê²½ìš°ë§Œ agentë¡œ ë³µê·€
        return "agent"

    # 11. ê·¸ë˜í”„ êµ¬ì„±
    workflow = StateGraph(AgentState)

    # ë…¸ë“œ ì¶”ê°€
    workflow.add_node("intent_classifier", intent_classifier_node)
    workflow.add_node("query_rewrite", query_rewrite_node)
    workflow.add_node("agent", agent_node)
    workflow.add_node("tools", tools_node_with_structured_data)

    # ì—£ì§€ ì—°ê²°
    workflow.add_edge(START, "intent_classifier")
    workflow.add_edge("intent_classifier", "query_rewrite")
    workflow.add_edge("query_rewrite", "agent")
    workflow.add_conditional_edges("agent", should_continue, ["tools", END])
    workflow.add_conditional_edges("tools", should_continue_after_tools, ["agent", END])  # ìˆ˜ì •

    # 12. ë©”ëª¨ë¦¬ ì²´í¬í¬ì¸íŠ¸
    memory = MemorySaver()

    # 13. ì»´íŒŒì¼
    app = workflow.compile(checkpointer=memory)

    print("[LangGraph] ì•± ìƒì„± ì™„ë£Œ")
    print(f"  - ë…¸ë“œ: intent_classifier â†’ query_rewrite â†’ agent â‡„ tools")
    print(f"  - ë„êµ¬: {len(tools)}ê°œ")

    return app
