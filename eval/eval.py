# -*- coding: utf-8 -*-
"""
í–‰ë™ìš”ë ¹ í‰ê°€ (LLM í‰ê°€ì ê¸°ë°˜)
- GPT-4ë¥¼ í‰ê°€ìë¡œ ì‚¬ìš©
- ì˜ë¯¸ì  í’ˆì§ˆ í‰ê°€
- êµ¬ì¡°í™”ëœ í”¼ë“œë°± ì œê³µ
"""
import json
import sys
from pathlib import Path
from dotenv import load_dotenv
from typing import Dict, List

project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

env_path = project_root / '.env'
load_dotenv(dotenv_path=env_path)

from backend.app.services.langgraph_agent import create_langgraph_app
from langchain_chroma import Chroma
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain_core.messages import HumanMessage

# ì¬ë‚œë³„ ì°¸ì¡° ê°€ì´ë“œë¼ì¸ (í‰ê°€ ê¸°ì¤€)
REFERENCE_GUIDELINES = {
    "earthquake": """
    ì§€ì§„ ë°œìƒ ì‹œ í–‰ë™ìš”ë ¹:
    1. ì‹¤ë‚´: ì±…ìƒì´ë‚˜ í…Œì´ë¸” ì•„ë˜ë¡œ ë“¤ì–´ê°€ ëª¸ì„ ë³´í˜¸í•˜ì„¸ìš”
    2. í”ë“¤ë¦¼ì´ ë©ˆì¶”ë©´ ê°€ìŠ¤ì™€ ì „ê¸°ë¥¼ ì°¨ë‹¨í•˜ì„¸ìš”
    3. ë¬¸ì„ ì—´ì–´ ì¶œêµ¬ë¥¼ í™•ë³´í•˜ì„¸ìš”
    4. ì‹¤ì™¸: ê±´ë¬¼ê³¼ ë–¨ì–´ì§„ ì•ˆì „í•œ ê³³ìœ¼ë¡œ ëŒ€í”¼í•˜ì„¸ìš”
    5. ì—˜ë¦¬ë² ì´í„° ì‚¬ìš© ê¸ˆì§€, ê³„ë‹¨ ì´ìš©
    """,
    "fire": """
    í™”ì¬ ë°œìƒ ì‹œ í–‰ë™ìš”ë ¹:
    1. "ë¶ˆì´ì•¼!" í¬ê²Œ ì™¸ì¹˜ë©° ì£¼ë³€ì— ì•Œë¦¬ì„¸ìš”
    2. ë‚®ì€ ìì„¸ë¡œ ì½”ì™€ ì…ì„ ë§‰ê³  ì´ë™í•˜ì„¸ìš”
    3. ì—°ê¸°ê°€ ë§ìœ¼ë©´ ë°”ë‹¥ìœ¼ë¡œ ê¸°ì–´ì„œ ì´ë™
    4. ì†Œí™”ê¸°ê°€ ìˆìœ¼ë©´ ì´ˆê¸° ì§„í™” ì‹œë„
    5. 119ì— ì‹ ê³ í•˜ê³  ì•ˆì „í•œ ê³³ìœ¼ë¡œ ëŒ€í”¼
    """,
    "flood": """
    í™ìˆ˜ ë°œìƒ ì‹œ í–‰ë™ìš”ë ¹:
    1. ë†’ì€ ê³³ìœ¼ë¡œ ì‹ ì†íˆ ëŒ€í”¼í•˜ì„¸ìš”
    2. ì¹¨ìˆ˜ëœ ë„ë¡œë‚˜ ì§€í•˜ê³µê°„ ì§„ì… ê¸ˆì§€
    3. ì „ê¸°/ê°€ìŠ¤ ì°¨ë‹¨ í›„ ëŒ€í”¼
    4. ë¬¼ì´ ë¹ ì§ˆ ë•Œê¹Œì§€ ì•ˆì „í•œ ê³³ì— ëŒ€ê¸°
    5. êµëŸ‰ì´ë‚˜ í•˜ì²œ ê·¼ì²˜ ì ‘ê·¼ ê¸ˆì§€
    """,
    "landslide": """
    ì‚°ì‚¬íƒœ ë°œìƒ ì‹œ í–‰ë™ìš”ë ¹:
    1. ì‚°ì—ì„œ ì¦‰ì‹œ í•˜ì‚°í•˜ì„¸ìš”
    2. ê²½ì‚¬ë©´ê³¼ ìˆ˜ì§ ë°©í–¥ìœ¼ë¡œ ëŒ€í”¼
    3. ê³„ê³¡ì´ë‚˜ ê²½ì‚¬ ì•„ë˜ ìœ„ì¹˜ í”¼í•˜ê¸°
    4. ì•ˆì „í•œ ê³ ì§€ëŒ€ë‚˜ ê±´ë¬¼ë¡œ ì´ë™
    """,
    "tsunami": """
    ì“°ë‚˜ë¯¸ ë°œìƒ ì‹œ í–‰ë™ìš”ë ¹:
    1. í•´ì•ˆì—ì„œ ìµœëŒ€í•œ ë©€ë¦¬, ë†’ì€ ê³³ìœ¼ë¡œ ëŒ€í”¼
    2. ì§€ì§„ í›„ í•´ì•ˆê°€ì—ì„œ ì¦‰ì‹œ ëŒ€í”¼
    3. ì“°ë‚˜ë¯¸ ê²½ë³´ ë°œë ¹ ì‹œ ì¦‰ê° ëŒ€í”¼
    4. ì²« íŒŒë„ê°€ ì§€ë‚˜ê°€ë„ ì•ˆì‹¬ ê¸ˆì§€ (ì—¬ëŸ¬ íŒŒë„ ì˜¬ ìˆ˜ ìˆìŒ)
    """,
    "storm": """
    í­í’ ë°œìƒ ì‹œ í–‰ë™ìš”ë ¹:
    1. ì‹¤ë‚´ë¡œ ëŒ€í”¼í•˜ì„¸ìš”
    2. ì°½ë¬¸ì—ì„œ ë©€ë¦¬ ë–¨ì–´ì§€ì„¸ìš”
    3. ì•¼ì™¸ í™œë™ ì¤‘ë‹¨
    4. ë‚ ì•„ê°ˆ ìˆ˜ ìˆëŠ” ë¬¼ê±´ ê³ ì •
    """,
    "typhoon": """
    íƒœí’ ë°œìƒ ì‹œ í–‰ë™ìš”ë ¹:
    1. ì™¸ì¶œ ìì œ, ì‹¤ë‚´ì— ë¨¸ë¬´ë¥´ì„¸ìš”
    2. ì°½ë¬¸ê³¼ ì¶œì…ë¬¸ì„ ì ê·¸ê³  í…Œì´í”„ë¡œ ë³´ê°•
    3. ê°•í’ì— ë‚ ì•„ê°ˆ ë¬¼ê±´ ì‹¤ë‚´ë¡œ ì´ë™
    4. ì €ì§€ëŒ€, í•´ì•ˆê°€ ì ‘ê·¼ ê¸ˆì§€
    5. íƒœí’ ëˆˆì´ ì§€ë‚˜ê°€ëŠ” ì¤‘ì—ë„ ì™¸ì¶œ ê¸ˆì§€
    """,
    "volcanic_ash": """
    í™”ì‚°ì¬ ë°œìƒ ì‹œ í–‰ë™ìš”ë ¹:
    1. ì‹¤ë‚´ì— ë¨¸ë¬´ë¥´ê³  ì°½ë¬¸ì„ ë‹«ìœ¼ì„¸ìš”
    2. ì™¸ì¶œ ì‹œ ë§ˆìŠ¤í¬ ì°©ìš© í•„ìˆ˜
    3. ëˆˆê³¼ í”¼ë¶€ ë³´í˜¸
    4. í™”ì‚°ì¬ê°€ ê°€ë¼ì•‰ì„ ë•Œê¹Œì§€ ëŒ€ê¸°
    """,
    "volcanic_eruption": """
    í™”ì‚° í­ë°œ ì‹œ í–‰ë™ìš”ë ¹:
    1. í™”ì‚°ì—ì„œ ìµœëŒ€í•œ ë©€ë¦¬ ëŒ€í”¼
    2. ìš©ì•” íë¦„ ë°©í–¥ ë°˜ëŒ€ë¡œ ì´ë™
    3. í™”ì‚°ì¬ë¡œë¶€í„° í˜¸í¡ê¸° ë³´í˜¸
    4. ë‚®ì€ ì§€ëŒ€ í”¼í•˜ê¸° (ê°€ìŠ¤ ì¶•ì  ìœ„í—˜)
    """,
    "wildfire": """
    ì‚°ë¶ˆ ë°œìƒ ì‹œ í–‰ë™ìš”ë ¹:
    1. ë°”ëŒì„ ë“±ì§€ê³  ëŒ€í”¼
    2. ë¶ˆê¸¸ ë°˜ëŒ€ ë°©í–¥ìœ¼ë¡œ ì‹ ì†íˆ ì´ë™
    3. ì –ì€ ìˆ˜ê±´ìœ¼ë¡œ ì½”ì™€ ì… ë³´í˜¸
    4. 119 ì‹ ê³ 
    """,
    "gas": """
    ê°€ìŠ¤ ëˆ„ì¶œ ì‹œ í–‰ë™ìš”ë ¹:
    1. í™˜ê¸°: ì°½ë¬¸ê³¼ ë¬¸ì„ ì¦‰ì‹œ ì—´ìœ¼ì„¸ìš”
    2. ë¶ˆê½ƒ/ì „ê¸° ìŠ¤ìœ„ì¹˜ ì ˆëŒ€ ê¸ˆì§€
    3. ê°€ìŠ¤ ë°¸ë¸Œ ì ê·¸ê¸°
    4. ëŒ€í”¼ í›„ 119 ì‹ ê³ 
    """,
    "dam": """
    ëŒ ë¶•ê´´ ì‹œ í–‰ë™ìš”ë ¹:
    1. ë†’ì€ ê³³ìœ¼ë¡œ ì¦‰ì‹œ ëŒ€í”¼
    2. í•˜ë¥˜ ì§€ì—­ ì‹ ì†íˆ ë²—ì–´ë‚˜ê¸°
    3. ë¼ë””ì˜¤ë¡œ ìƒí™© í™•ì¸
    """,
    "radiation": """
    ë°©ì‚¬ëŠ¥ ëˆ„ì¶œ ì‹œ í–‰ë™ìš”ë ¹:
    1. ì‹¤ë‚´ë¡œ ëŒ€í”¼, ì°½ë¬¸ê³¼ ë¬¸ ë‹«ê¸°
    2. ì™¸ë¶€ ê³µê¸° ì°¨ë‹¨
    3. ìš”ì˜¤ë“œ ë³µìš© (ë‹¹êµ­ ì§€ì‹œ ì‹œ)
    4. ëŒ€í”¼ ì§€ì‹œ ë”°ë¥´ê¸°
    """
}

class LLMEvaluator:
    """LLM ê¸°ë°˜ ì‘ë‹µ í’ˆì§ˆ í‰ê°€ì"""
    
    def __init__(self, model_name: str = "gpt-4o"):
        self.llm = ChatOpenAI(model=model_name, temperature=0)
    
    def evaluate_response(
        self, 
        query: str, 
        response: str, 
        expected_disaster: str
    ) -> Dict:
        """
        ì‘ë‹µì„ í‰ê°€í•˜ê³  êµ¬ì¡°í™”ëœ ê²°ê³¼ ë°˜í™˜
        
        Returns:
            {
                "total_score": 85,
                "relevance_score": 55,  # ê´€ë ¨ì„± (60ì  ë§Œì )
                "quality_score": 18,     # í’ˆì§ˆ (20ì  ë§Œì )
                "purity_score": 12,      # ìˆœìˆ˜ë„ (20ì  ë§Œì )
                "feedback": "ìƒì„¸ í”¼ë“œë°±...",
                "strengths": ["ê°•ì 1", "ê°•ì 2"],
                "weaknesses": ["ì•½ì 1", "ì•½ì 2"]
            }
        """
        reference = REFERENCE_GUIDELINES.get(
            expected_disaster.lower().replace(" ", "_"), 
            "í•´ë‹¹ ì¬ë‚œì— ëŒ€í•œ ì°¸ì¡° ê°€ì´ë“œë¼ì¸ì´ ì—†ìŠµë‹ˆë‹¤."
        )
        
        evaluation_prompt = f"""ë‹¹ì‹ ì€ ì¬ë‚œ ì•ˆì „ êµìœ¡ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ë‹¤ìŒ ì‘ë‹µì„ í‰ê°€í•´ì£¼ì„¸ìš”.

**ì‚¬ìš©ì ì§ˆë¬¸:**
{query}

**ê¸°ëŒ€ ì¬ë‚œ ìœ í˜•:**
{expected_disaster}

**ì°¸ì¡° ê°€ì´ë“œë¼ì¸ (ì •ë‹µ ê¸°ì¤€):**
{reference}

**ì‹¤ì œ ì‘ë‹µ:**
{response}

---

ë‹¤ìŒ 3ê°€ì§€ ê¸°ì¤€ìœ¼ë¡œ í‰ê°€í•˜ì„¸ìš”:

1. **ê´€ë ¨ì„± (60ì )**: ê¸°ëŒ€ ì¬ë‚œì˜ í–‰ë™ìš”ë ¹ì´ ì–¼ë§ˆë‚˜ í¬í•¨ë˜ì—ˆëŠ”ê°€?
   - í•µì‹¬ í–‰ë™ìš”ë ¹ í¬í•¨: 40-60ì 
   - ì¼ë¶€ ê´€ë ¨ ì •ë³´: 20-39ì 
   - ê±°ì˜ ë¬´ê´€: 0-19ì 

2. **í’ˆì§ˆ (20ì )**: ë‚´ìš©ì´ ì–¼ë§ˆë‚˜ êµ¬ì²´ì ì´ê³  ì‹¤ìš©ì ì¸ê°€?
   - ë§¤ìš° êµ¬ì²´ì ì´ê³  ì‹¤í–‰ ê°€ëŠ¥: 16-20ì 
   - ì ì ˆí•œ ìˆ˜ì¤€: 11-15ì 
   - ëª¨í˜¸í•˜ê±°ë‚˜ ë¶ˆì¶©ë¶„: 0-10ì 

3. **ìˆœìˆ˜ë„ (20ì )**: ë‹¤ë¥¸ ì¬ë‚œ ì •ë³´ê°€ ì„ì´ì§€ ì•Šì•˜ëŠ”ê°€?
   - ì˜¤ì—¼ ì—†ìŒ: 16-20ì 
   - ì•½ê°„ì˜ ê´€ë ¨ ì—†ëŠ” ë‚´ìš©: 11-15ì 
   - ì‹¬ê°í•œ ì˜¤ì—¼ (ë‹¤ë¥¸ ì¬ë‚œ ê°€ì´ë“œ í˜¼ì…): 0-10ì 

**JSON í˜•ì‹ìœ¼ë¡œë§Œ ë‹µë³€í•˜ì„¸ìš”:**
{{
  "relevance_score": 55,
  "quality_score": 18,
  "purity_score": 12,
  "total_score": 85,
  "feedback": "ì „ì²´ì ì¸ í‰ê°€ ìš”ì•½ (2-3ë¬¸ì¥)",
  "strengths": ["ê°•ì 1", "ê°•ì 2"],
  "weaknesses": ["ì•½ì 1", "ì•½ì 2"],
  "key_missing": ["ëˆ„ë½ëœ í•µì‹¬ ì •ë³´1", "ëˆ„ë½ëœ í•µì‹¬ ì •ë³´2"]
}}

ì¶”ê°€ ì„¤ëª… ì—†ì´ JSONë§Œ ì¶œë ¥í•˜ì„¸ìš”."""

        try:
            result = self.llm.invoke(evaluation_prompt)
            content = result.content.strip()
            
            # JSON íŒŒì‹± (ë§ˆí¬ë‹¤ìš´ ì½”ë“œ ë¸”ë¡ ì œê±°)
            if content.startswith("```"):
                content = content.split("```")[1]
                if content.startswith("json"):
                    content = content[4:]
                content = content.strip()
            
            evaluation = json.loads(content)
            
            # ì ìˆ˜ ê²€ì¦
            evaluation["relevance_score"] = max(0, min(60, evaluation.get("relevance_score", 0)))
            evaluation["quality_score"] = max(0, min(20, evaluation.get("quality_score", 0)))
            evaluation["purity_score"] = max(0, min(20, evaluation.get("purity_score", 0)))
            evaluation["total_score"] = (
                evaluation["relevance_score"] + 
                evaluation["quality_score"] + 
                evaluation["purity_score"]
            )
            
            return evaluation
            
        except json.JSONDecodeError as e:
            print(f"âš ï¸  JSON íŒŒì‹± ì‹¤íŒ¨: {e}")
            print(f"ì‘ë‹µ ë‚´ìš©: {content[:200]}...")
            return {
                "total_score": 0,
                "relevance_score": 0,
                "quality_score": 0,
                "purity_score": 0,
                "feedback": "í‰ê°€ ì‹¤íŒ¨ - JSON íŒŒì‹± ì˜¤ë¥˜",
                "strengths": [],
                "weaknesses": ["í‰ê°€ ì‹œìŠ¤í…œ ì˜¤ë¥˜"],
                "key_missing": [],
                "error": str(e)
            }
        except Exception as e:
            print(f"âš ï¸  í‰ê°€ ì˜¤ë¥˜: {e}")
            return {
                "total_score": 0,
                "relevance_score": 0,
                "quality_score": 0,
                "purity_score": 0,
                "feedback": f"í‰ê°€ ì‹¤íŒ¨: {str(e)}",
                "strengths": [],
                "weaknesses": ["í‰ê°€ ì‹œìŠ¤í…œ ì˜¤ë¥˜"],
                "key_missing": [],
                "error": str(e)
            }


def evaluate_with_llm(test_path: str, langgraph_app):
    """LLM ê¸°ë°˜ í–‰ë™ìš”ë ¹ í‰ê°€"""
    
    evaluator = LLMEvaluator()
    
    with open(test_path, 'r', encoding='utf-8') as f:
        test_cases = json.load(f)
    
    results = {
        "total": len(test_cases),
        "excellent": 0,      # 90ì  ì´ìƒ
        "good": 0,           # 70-89ì 
        "acceptable": 0,     # 50-69ì 
        "poor": 0,           # 50ì  ë¯¸ë§Œ
        "avg_total_score": 0,
        "avg_relevance_score": 0,
        "avg_quality_score": 0,
        "avg_purity_score": 0,
        "details": []
    }
    
    total_scores = []
    relevance_scores = []
    quality_scores = []
    purity_scores = []
    
    for idx, case in enumerate(test_cases):
        query = case["query"]
        expected_disaster = case["expected_disaster_type"]
        
        print(f"\n{'='*70}")
        print(f"[í…ŒìŠ¤íŠ¸ {idx+1}/{len(test_cases)}] {query}")
        print(f"ê¸°ëŒ€ ì¬ë‚œ: {expected_disaster}")
        print('='*70)
        
        try:
            # LangGraph ì‹¤í–‰
            response = langgraph_app.invoke(
                {"messages": [HumanMessage(content=query)]},
                config={"configurable": {"thread_id": f"test_{idx}"}}
            )
            
            # ì‘ë‹µ ì¶”ì¶œ
            response_text = ""
            if "messages" in response and response["messages"]:
                last_msg = response["messages"][-1]
                if hasattr(last_msg, 'content'):
                    response_text = last_msg.content
            
            print(f"\nğŸ“ ì‘ë‹µ ê¸¸ì´: {len(response_text)} ì")
            print(f"ğŸ“ ì‘ë‹µ ë¯¸ë¦¬ë³´ê¸°: {response_text[:150]}...")
            
            # LLM í‰ê°€
            print("\nğŸ¤– LLM í‰ê°€ ì¤‘...")
            evaluation = evaluator.evaluate_response(
                query, 
                response_text, 
                expected_disaster
            )
            
            # ì ìˆ˜ ì§‘ê³„
            total_score = evaluation["total_score"]
            total_scores.append(total_score)
            relevance_scores.append(evaluation["relevance_score"])
            quality_scores.append(evaluation["quality_score"])
            purity_scores.append(evaluation["purity_score"])
            
            # ë“±ê¸‰ ë¶„ë¥˜
            if total_score >= 90:
                results["excellent"] += 1
                grade = "ğŸ† ìš°ìˆ˜"
            elif total_score >= 70:
                results["good"] += 1
                grade = "âœ… ì–‘í˜¸"
            elif total_score >= 50:
                results["acceptable"] += 1
                grade = "âš ï¸  ë³´í†µ"
            else:
                results["poor"] += 1
                grade = "âŒ ë¯¸í¡"
            
            # ê²°ê³¼ ì¶œë ¥
            print(f"\nğŸ“Š í‰ê°€ ê²°ê³¼: {grade}")
            print(f"   ì´ì : {total_score}/100")
            print(f"   - ê´€ë ¨ì„±: {evaluation['relevance_score']}/60")
            print(f"   - í’ˆì§ˆ: {evaluation['quality_score']}/20")
            print(f"   - ìˆœìˆ˜ë„: {evaluation['purity_score']}/20")
            print(f"\nğŸ’¬ í”¼ë“œë°±: {evaluation['feedback']}")
            
            if evaluation.get('strengths'):
                print(f"\nâœ¨ ê°•ì :")
                for strength in evaluation['strengths']:
                    print(f"   â€¢ {strength}")
            
            if evaluation.get('weaknesses'):
                print(f"\nâš ï¸  ì•½ì :")
                for weakness in evaluation['weaknesses']:
                    print(f"   â€¢ {weakness}")
            
            if evaluation.get('key_missing'):
                print(f"\nğŸ“Œ ëˆ„ë½ëœ í•µì‹¬ ì •ë³´:")
                for missing in evaluation['key_missing']:
                    print(f"   â€¢ {missing}")
            
            # ìƒì„¸ ê²°ê³¼ ì €ì¥
            results["details"].append({
                "query": query,
                "expected_disaster": expected_disaster,
                "response": response_text,
                "response_preview": response_text[:300],
                "evaluation": evaluation,
                "grade": grade
            })
            
        except Exception as e:
            print(f"\nâŒ ì—ëŸ¬ ë°œìƒ: {e}")
            import traceback
            traceback.print_exc()
            
            results["poor"] += 1
            results["details"].append({
                "query": query,
                "expected_disaster": expected_disaster,
                "error": str(e),
                "evaluation": {
                    "total_score": 0,
                    "relevance_score": 0,
                    "quality_score": 0,
                    "purity_score": 0,
                    "feedback": f"ì‹œìŠ¤í…œ ì˜¤ë¥˜: {str(e)}"
                },
                "grade": "âŒ ì˜¤ë¥˜"
            })
    
    # í‰ê·  ê³„ì‚°
    if total_scores:
        results["avg_total_score"] = round(sum(total_scores) / len(total_scores), 2)
        results["avg_relevance_score"] = round(sum(relevance_scores) / len(relevance_scores), 2)
        results["avg_quality_score"] = round(sum(quality_scores) / len(quality_scores), 2)
        results["avg_purity_score"] = round(sum(purity_scores) / len(purity_scores), 2)
    
    return results


if __name__ == "__main__":
    print("\n" + "="*70)
    print("ğŸš€ LLM ê¸°ë°˜ í–‰ë™ìš”ë ¹ í‰ê°€ ì‹œìŠ¤í…œ")
    print("="*70)
    
    print("\n[1/3] ë²¡í„°ìŠ¤í† ì–´ ë¡œë“œ ì¤‘...")
    vectorstore = Chroma(
        persist_directory=str(project_root / "chroma_db"),
        embedding_function=OpenAIEmbeddings()
    )
    print("âœ… ë²¡í„°ìŠ¤í† ì–´ ë¡œë“œ ì™„ë£Œ")
    
    print("\n[2/3] LangGraph ì•± ìƒì„± ì¤‘...")
    langgraph_app = create_langgraph_app(vectorstore)
    print("âœ… LangGraph ì•± ìƒì„± ì™„ë£Œ")
    
    print("\n[3/3] í‰ê°€ ì‹œì‘...")
    test_path = project_root / "eval" / "guideline_test.json"
    results = evaluate_with_llm(str(test_path), langgraph_app)
    
    # ìµœì¢… ê²°ê³¼ ì¶œë ¥
    print("\n" + "="*70)
    print("ğŸ“Š ìµœì¢… í‰ê°€ ê²°ê³¼")
    print("="*70)
    print(f"ì´ í…ŒìŠ¤íŠ¸: {results['total']}ê°œ")
    print(f"\në“±ê¸‰ë³„ ë¶„í¬:")
    print(f"  ğŸ† ìš°ìˆ˜ (90-100ì ): {results['excellent']}ê°œ ({results['excellent']/results['total']*100:.1f}%)")
    print(f"  âœ… ì–‘í˜¸ (70-89ì ):  {results['good']}ê°œ ({results['good']/results['total']*100:.1f}%)")
    print(f"  âš ï¸  ë³´í†µ (50-69ì ):  {results['acceptable']}ê°œ ({results['acceptable']/results['total']*100:.1f}%)")
    print(f"  âŒ ë¯¸í¡ (0-49ì ):   {results['poor']}ê°œ ({results['poor']/results['total']*100:.1f}%)")
    print(f"\ní‰ê·  ì ìˆ˜:")
    print(f"  ì´ì :   {results['avg_total_score']:.2f}/100")
    print(f"  ê´€ë ¨ì„±: {results['avg_relevance_score']:.2f}/60")
    print(f"  í’ˆì§ˆ:   {results['avg_quality_score']:.2f}/20")
    print(f"  ìˆœìˆ˜ë„: {results['avg_purity_score']:.2f}/20")
    print("="*70)
    
    # ê²°ê³¼ ì €ì¥
    output_path = project_root / "eval" / "guideline_results_llm.json"
    output_path.parent.mkdir(parents=True, exist_ok=True)
    
    with open(output_path, 'w', encoding='utf-8') as f:
        json.dump(results, f, ensure_ascii=False, indent=2)
    
    print(f"\nâœ… ê²°ê³¼ ì €ì¥ ì™„ë£Œ: {output_path}")
    print("\nğŸ’¡ ê° í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ì˜ ìƒì„¸ í”¼ë“œë°±ì´ JSON íŒŒì¼ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")